{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration\n",
    "\n",
    "This notebook logs exploratory results on adding teleportation on MCC with state coverage visualisation. Used for rough initial exploration.\n",
    "\n",
    "21/01/2024\n",
    "- Naive teleportation to argmax works\n",
    "- Longer episodes are better than shorter\n",
    "- Different intrinsic rewards show significantly different behavior\n",
    "- Even naively, general improvement over pure intrinsic\n",
    "- Fails to beat intrinsic + extrinsic: perhaps this is due to negative extrinsic reward revealing data on target? Not comparable, and I think fully explored in that reward shifting paper\n",
    "- Keeps teleporting to same target\n",
    "- This may be a problem with DDPG\n",
    "\n",
    "\n",
    "28/01/2024\n",
    "- Probabilistic teleportation work well\n",
    "- Environment reset stochasticity is important\n",
    "- Time limit aware Q functions are difficult to train!\n",
    "- Proposal: Dynamic Truncation!\n",
    "\n",
    "4/02/2024\n",
    "- ICM and RND leads to inherently different results - RND should be prioritised\n",
    "- CATS fails to improve over baseline on RND with fixed reset, but does in ICM. After reset, the new trajectory follows the previous trajectory too closely, while resetting from the start leads to more divergence across the entire episode (and hence more exploration)\n",
    "- Fixing the reset states leads to improved analysis\n",
    "- Policy function gets stuck in the local minima of the Q function\n",
    "- Analyse DQN instead? Skip parametrized policy function and use an approximator?? Maybe implement QT-opt https://arxiv.org/pdf/1806.10293.pdf. This may be important to obtain interesting experiment results, since on MCC the policy generally fails to follow the critic even on large learning rates (why??)\n",
    "\n",
    "11/02/2024\n",
    "- Ensemble bootstrapping (Thompson sampling) seems to have uncertain impact over baseline, maybe slightly positive?\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Confidence Bounds (How? Without latent density estimator?)\n",
    "- Termination as an action\n",
    "- Epsilon greedy\n",
    "- Time aware exploration\n",
    "\n",
    "Known Failure Modes\n",
    "- Teleporting to the end of the episode, and immediately truncating\n",
    "- \n",
    "\n",
    "Ideas\n",
    "- Bootstrapped Q value estimate for confidence bound guided estimation?\n",
    "\n",
    "Interesting observations\n",
    "- Qt_opt directly on critic, rather than target network explores faster??\n",
    "\n",
    "Reward normalisation messes up learning to reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with your own\n",
    "# %env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/markhaoxiang/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
    "\n",
    "# Define Imports and shared training information\n",
    "\n",
    "import importlib\n",
    "# Std\n",
    "import copy\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "# Evaluation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Curiosity\n",
    "import curiosity\n",
    "from curiosity.experience import Transition\n",
    "from curiosity.util.util import *\n",
    "import cats\n",
    "from cats import CatsExperiment\n",
    "from evaluation import *\n",
    "importlib.reload(cats)\n",
    "importlib.reload(curiosity)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "COLLECTION_STEPS = 4096\n",
    "MAX_EPISODE_STEPS = 500\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "#ENV = \"Pendulum-v1\"\n",
    "#ENV = \"HalfCheetah-v4\"\n",
    "cfg = DictConfig({\n",
    "    \"env\": {\n",
    "        \"name\": ENV,\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"type\": \"experience_replay\",\n",
    "        \"normalise_observation\": True\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"initial_collection_size\": 256,\n",
    "        \"total_frames\": COLLECTION_STEPS,\n",
    "        \"minibatch_size\": 128\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"type\": \"qt_opt\",\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"lr\": 0.01,\n",
    "        \"update_frequency\": 1,\n",
    "        \"clip_grad_norm\": 1,\n",
    "        \"ensemble_number\": 5,\n",
    "        \"actor\": {\n",
    "            \"features\": 128\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"features\": 128\n",
    "        }\n",
    "    },\n",
    "    \"intrinsic\": {\n",
    "        \"type\": \"rnd\",\n",
    "        \"encoding_size\": 32,\n",
    "        \"lr\": 0.0003,\n",
    "        \"int_coef\": 1, \n",
    "        \"ext_coef\": 2,\n",
    "        \"reward_normalisation\": True,\n",
    "        \"normalised_obs_clip\": 5\n",
    "    },\n",
    "    \"noise\": {\n",
    "        \"scale\": 0.1,\n",
    "        \"beta\": 0\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = CatsExperiment(\n",
    "    cfg=cfg,\n",
    "    collection_steps=COLLECTION_STEPS,\n",
    "    device=DEVICE,\n",
    "    enable_policy_sampling=True,\n",
    "    teleport_strategy=None,\n",
    "    environment_action_noise=0,\n",
    "    seed=0\n",
    ")\n",
    "experiment.run()\n",
    "\n",
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(entropy_memory(experiment.memory))\n",
    "visualise_memory(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping Intrinsic Motivation\n",
    "\n",
    "Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "baseline_statistics = {\n",
    "    'total_intrinsic_reward': [],\n",
    "    'newly_collected_intrinsic_reward': [],\n",
    "    'entropy': []\n",
    "}\n",
    "bootstrapping_statistics = copy.deepcopy(baseline_statistics)\n",
    "\n",
    "# Baseline\n",
    "for seed in range(0, N):\n",
    "    experiment = CatsExperiment(\n",
    "        cfg=cfg,\n",
    "        collection_steps=COLLECTION_STEPS,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        enable_policy_sampling=False\n",
    "    )\n",
    "    experiment.run()\n",
    "    for k,v in experiment.log.items():\n",
    "        baseline_statistics[k].append(v)\n",
    "    baseline_statistics['entropy'].append(entropy_memory(experiment.memory))\n",
    "\n",
    "for seed in range(0, N):\n",
    "    experiment = CatsExperiment(\n",
    "        cfg=cfg,\n",
    "        collection_steps=COLLECTION_STEPS,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        enable_policy_sampling=True\n",
    "    )\n",
    "    experiment.run()\n",
    "    for k,v in experiment.log.items():\n",
    "        bootstrapping_statistics[k].append(v)\n",
    "    bootstrapping_statistics['entropy'].append(entropy_memory(experiment.memory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline\")\n",
    "print(f\"Total Intrinsic Reward: {sum(baseline_statistics['total_intrinsic_reward'])/N}\")\n",
    "print(f\"Collected Intrinsic Reward: {sum(baseline_statistics['newly_collected_intrinsic_reward'])/N}\")\n",
    "print(f\"Entropy: {sum(baseline_statistics['entropy'])/N}\")\n",
    "print(\"Bootstrapped\")\n",
    "print(f\"Total Intrinsic Reward: {sum(bootstrapping_statistics['total_intrinsic_reward'])/N}\")\n",
    "print(f\"Collected Intrinsic Reward: {sum(bootstrapping_statistics['newly_collected_intrinsic_reward'])/N}\")\n",
    "print(f\"Entropy: {sum(bootstrapping_statistics['entropy'])/N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Aleatoric Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 # Number of repeats\n",
    "COLLECTION_STEPS = 4096\n",
    "SEED = 0\n",
    "\n",
    "baseline_statistics = [] \n",
    "\n",
    "for uncertainty in np.linspace(0, 0.5, 11):\n",
    "    statistics = {}\n",
    "    statistics[\"entropy\"] = []\n",
    "    statistics[\"log\"] = []\n",
    "    \n",
    "    experiment = CatsExperiment(\n",
    "        cfg=cfg,\n",
    "        collection_steps=COLLECTION_STEPS,\n",
    "        seed=SEED,\n",
    "        device=DEVICE,\n",
    "        enable_policy_sampling=False,\n",
    "        environment_action_noise=uncertainty\n",
    "    )\n",
    "    experiment.run()\n",
    "    statistics[\"entropy\"].append(entropy_memory(experiment.memory))\n",
    "    statistics[\"log\"].append(experiment.log)\n",
    "\n",
    "    baseline_statistics.append(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_memory(experiment.env, (experiment.memory, \"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Runtime - 7 minutes\n",
    "\n",
    "statistics = {\n",
    "    \"entropy\": [],\n",
    "    \"log\": []\n",
    "}\n",
    "SEED = 0\n",
    "\n",
    "for max_episode_length in range(100, 1001, 50):\n",
    "    experiment = CatsExperiment(\n",
    "        cfg=cfg,\n",
    "        collection_steps=5000,\n",
    "        max_episode_steps=max_episode_length,\n",
    "        fixed_reset=False,\n",
    "        seed=SEED,\n",
    "        device=DEVICE, \n",
    "    )\n",
    "    experiment.run()\n",
    "    statistics[\"entropy\"].append(entropy_memory(experiment.memory))\n",
    "    statistics[\"log\"].append(experiment.log)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(list(range(100, 1001, 50)), statistics[\"entropy\"])\n",
    "ax.set_title(\"Exploration with Episode Length\")\n",
    "ax.set_xlabel(\"Max episode length\")\n",
    "ax.set_ylabel(\"Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn to Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "experiment_cfg = copy.deepcopy(cfg)\n",
    "experiment_cfg.noise.scale = [0.1, 0.01]\n",
    "experiment = CatsExperiment(\n",
    "    cfg=experiment_cfg,\n",
    "    collection_steps=COLLECTION_STEPS,\n",
    "    #max_episode_steps=300,\n",
    "    max_episode_steps=math.inf,\n",
    "    reset_as_an_action=200,\n",
    "    fixed_reset=False,\n",
    "    seed=SEED,\n",
    "    device=DEVICE\n",
    ")\n",
    "experiment.run()\n",
    "\n",
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(entropy_memory(experiment.memory))\n",
    "visualise_memory(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_cfg = copy.deepcopy(cfg)\n",
    "experiment_cfg.noise.scale = [0.1, 0.01]\n",
    "experiment = CatsExperiment(\n",
    "    cfg=experiment_cfg,\n",
    "    collection_steps=COLLECTION_STEPS,\n",
    "    max_episode_steps=math.inf,\n",
    "    reset_as_an_action=True,\n",
    "    seed=SEED,\n",
    "    device=DEVICE\n",
    ")\n",
    "experiment.run()\n",
    "\n",
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(entropy_memory(experiment.memory))\n",
    "visualise_memory(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code (To be moved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some deprecated code\n",
    "\n",
    "\n",
    "# # Learn To Truncate\n",
    "# \n",
    "# class CATS_Truncate(Experiment):\n",
    "# \n",
    "#     def __init__(self,\n",
    "#                  cfg,\n",
    "#                  max_episode_steps: Optional[int] = None,\n",
    "#                  death_is_not_the_end: bool = True,\n",
    "#                  fixed_reset: bool = False,\n",
    "#                  storage_size: float = 100,\n",
    "#                  epsilon: float = 0.1):\n",
    "#         super().__init__(cfg, max_episode_steps, death_is_not_the_end, fixed_reset)\n",
    "# \n",
    "#         self.storage_size = storage_size\n",
    "#         # Recently explored trajectory\n",
    "#         self.trajectory = []\n",
    "# \n",
    "#         # Current time step\n",
    "#         self.trajectory_index = 0\n",
    "#         self.trajectory_counter = 0\n",
    "#         # Target Timestep\n",
    "#         self.teleport_index = 0\n",
    "#         # Reset epsilon\n",
    "#         self.epsilon = epsilon\n",
    "#         \n",
    "#         # Environment deepcopies\n",
    "#         self.state = None\n",
    "#         self.quicksaves = []\n",
    "# \n",
    "#         # RNG\n",
    "#         self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "# \n",
    "#         self.log[\"teleport_targets\"] = []\n",
    "#         self.log[\"teleport_targets_observations\"] = []\n",
    "#         self.log[\"latest_trajectory\"] = []\n",
    "# \n",
    "#     def _build_env(self, max_episode_steps=None):\n",
    "#         super()._build_env(max_episode_steps)\n",
    "# \n",
    "#         # Learn to Truncate\n",
    "#         self.env = self.env.env\n",
    "# \n",
    "#     def _teleport_selection(self, V):\n",
    "#         #V = V**2\n",
    "#             # Argmax\n",
    "#         teleport_index = torch.argmax(V).item()\n",
    "#             # Probability Matching\n",
    "#         #p = V / V.sum()\n",
    "#         # pt = self.np_rng.random()\n",
    "#         # pc = 0\n",
    "#         # for i, pi in enumerate(p):\n",
    "#         #     pc += pi\n",
    "#         #     if pc >= pt or i == len(p) - 1:\n",
    "#         #         teleport_index = i\n",
    "#         #         break\n",
    "#         #     # TODO: Upper Confidence bound\n",
    "#         return teleport_index\n",
    "# \n",
    "#     def _reset(self, V):\n",
    "#         if self.fixed_reset:\n",
    "#             return\n",
    "#             # Epsilon Greedy Reset\n",
    "#         #if torch.rand(1) < epsilon:\n",
    "#         #    obs, infos = collector.env.reset()\n",
    "#         #    resets[0] = collector.env\n",
    "#         #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "#         #    teleport_index = 0\n",
    "#             # Reset Buffer\n",
    "#         reset_buffer = []\n",
    "#         reset_buffer_obs = []\n",
    "#         for i in range(10):\n",
    "#             obs, info = self.collector.env.reset(seed=int(self.np_rng.integers(65536)))\n",
    "#             reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "#             reset_buffer_obs.append(obs)\n",
    "#         reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs, dtype=np.float32), device=DEVICE)\n",
    "#         V_r = self.V(reset_buffer_obs)\n",
    "#         best_reset_index = torch.argmax(V_r).item()\n",
    "#         #if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "#         if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "#             self.collector.env = reset_buffer[best_reset_index]\n",
    "#             self.quicksaves[0] = self.collector.env\n",
    "#             self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "#             self.teleport_index = 0\n",
    "# \n",
    "#     def run(self):\n",
    "#         if self.fixed_reset:\n",
    "#             self.collector.env.reset(seed=self.cfg.seed)\n",
    "# \n",
    "#         early_start_transitions = self.collector.early_start(cfg.train.initial_collection_size)\n",
    "#         for t in early_start_transitions:\n",
    "#             self._update_memory(*t)\n",
    "# \n",
    "#         batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "#         self.intrinsic.initialise(Transition(*batch), aux)\n",
    "# \n",
    "#         # Main Loop\n",
    "#         if self.fixed_reset:\n",
    "#             self.collector.env.reset(seed=self.cfg.seed)\n",
    "#         else:\n",
    "#             self.collector.env.reset()\n",
    "# \n",
    "#         self.state = copy.deepcopy(self.collector.env)\n",
    "#         newly_collected_intrinsic_reward = 0\n",
    "#         for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "#             obs, action, reward, n_obs, terminated, _ = self.collector.collect(n=1)[-1]\n",
    "# \n",
    "#             newly_collected_intrinsic_reward += self.intrinsic.reward(Transition(\n",
    "#                 torch.tensor(obs, device=DEVICE).unsqueeze(0),\n",
    "#                 torch.tensor(action, device=DEVICE).unsqueeze(0),\n",
    "#                 torch.tensor(reward, device=DEVICE).unsqueeze(0),\n",
    "#                 torch.tensor(n_obs, device=DEVICE).unsqueeze(0),\n",
    "#                 torch.tensor(terminated, device=DEVICE).unsqueeze(0)\n",
    "#             ))[2].item()\n",
    "# \n",
    "#             if self.trajectory_counter == self.storage_size - 1:\n",
    "#                 truncated = True\n",
    "#             else:\n",
    "#                 truncated = False\n",
    "#             # Time Limit Normalisation\n",
    "#             # obs, n_obs = copy.deepcopy(obs), copy.deepcopy(n_obs)\n",
    "#             # obs[-1] = obs[-1] / self.env.spec.max_episode_steps\n",
    "#             # n_obs[-1] = n_obs[-1] / self.env.spec.max_episode_steps\n",
    "# \n",
    "#             # Update trajectory\n",
    "#             self.quicksaves.append(self.state)\n",
    "#             self.trajectory.append(obs)\n",
    "#             self.state = copy.deepcopy(self.collector.env)\n",
    "#             self.trajectory_index += 1\n",
    "#             self.trajectory_counter += 1\n",
    "# \n",
    "#             # Manage Teleportation\n",
    "#             if truncated or terminated:\n",
    "#                 # Calculate Value\n",
    "#                 V = self.V(torch.tensor(self.trajectory,device=DEVICE))\n",
    "#                 # Teleportation Selection\n",
    "#                 with torch.no_grad():\n",
    "#                     self.teleport_index = self._teleport_selection(V)\n",
    "#                 # Resets\n",
    "#                 self._reset(V)\n",
    "# \n",
    "#                 # Teleport\n",
    "#                 self.collector.env = self.quicksaves[self.teleport_index]\n",
    "#                 self.collector.obs = self.trajectory[self.teleport_index]\n",
    "#                 self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "# \n",
    "#                 self.trajectory_index = self.teleport_index\n",
    "#                 self.trajectory_counter = 0\n",
    "#                 self.trajectory = self.trajectory[:self.teleport_index+1]\n",
    "#                 self.quicksaves = self.quicksaves[:self.teleport_index+1]\n",
    "#                 self.state = copy.deepcopy(self.collector.env)\n",
    "# \n",
    "#                 # Log\n",
    "#                 self.log[\"teleport_targets\"].append(self.teleport_index)\n",
    "#                 self.log[\"teleport_targets_observations\"].append(self.collector.obs)\n",
    "#                 self.log[\"latest_trajectory\"] = self.trajectory\n",
    "# \n",
    "#             # Update Memory\n",
    "#             self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "#             \n",
    "#             # Remaining RL Update\n",
    "#             batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "#             batch = Transition(*batch)\n",
    "#             if self.death_is_not_the_end:\n",
    "#                 batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "# \n",
    "# \n",
    "#             r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "#             self.intrinsic.update(batch, aux, step=step)\n",
    "# \n",
    "#             batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "#             self.algorithm.update(batch, aux, step=step)\n",
    "# \n",
    "#             # Log\n",
    "#             self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n",
    "# \n",
    "#         print(newly_collected_intrinsic_reward)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
