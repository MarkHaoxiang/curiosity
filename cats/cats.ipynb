{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration\n",
    "\n",
    "This notebook logs exploratory results on adding teleportation on MCC with state coverage visualisation\n",
    "\n",
    "21/01/2024\n",
    "- Naive teleportation to argmax works\n",
    "- Longer episodes are better than shorter\n",
    "- Different intrinsic rewards show significantly different behavior\n",
    "- Even naively, general improvement over pure intrinsic\n",
    "- Fails to beat intrinsic + extrinsic: perhaps this is due to negative extrinsic reward revealing data on target? Not comparable, and I think fully explored in that reward shifting paper\n",
    "- Keeps teleporting to same target\n",
    "- This may be a problem with DDPG\n",
    "\n",
    "\n",
    "28/01/2024\n",
    "- Probabilistic teleportation work well\n",
    "- Environment reset stochasticity is important\n",
    "- Time limit aware Q functions are difficult to train!\n",
    "- Proposal: Dynamic Truncation!\n",
    "\n",
    "4/02/2024\n",
    "- ICM and RND leads to inherently different results - RND should be prioritised\n",
    "- CATS fails to improve over baseline on RND with fixed reset, but does in ICM. After reset, the new trajectory follows the previous trajectory too closely, while resetting from the start leads to more divergence across the entire episode (and hence more exploration)\n",
    "- Fixing the reset states leads to improved analysis\n",
    "- Policy function gets stuck in the local minima of the Q function\n",
    "- Analyse DQN instead? Skip parametrized policy function and use an approximator?? Maybe implement QT-opt https://arxiv.org/pdf/1806.10293.pdf. This may be important to obtain interesting experiment results, since on MCC the policy generally fails to follow the critic even on large learning rates (why??)\n",
    "\n",
    "11/02/2024\n",
    "- Ensemble bootstrapping (Thompson sampling) seems to have uncertain impact over baseline, maybe slightly positive?\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Confidence Bounds (How? Without latent density estimator?)\n",
    "- Termination as an action\n",
    "- Epsilon greedy\n",
    "- Time aware exploration\n",
    "\n",
    "Known Failure Modes\n",
    "- Teleporting to the end of the episode, and immediately truncating\n",
    "- \n",
    "\n",
    "Ideas\n",
    "- Bootstrapped Q value estimate for confidence bound guided estimation?\n",
    "\n",
    "Interesting observations\n",
    "- Qt_opt directly on critic, rather than target network explores faster??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Imports and shared training information\n",
    "\n",
    "# Std\n",
    "import copy\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from omegaconf import DictConfig\n",
    "import gymnasium as gym\n",
    "\n",
    "# Evaluation\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Curiosity\n",
    "from curiosity.experience import Transition\n",
    "from curiosity.experience.collector import GymCollector\n",
    "from curiosity.policy import ColoredNoisePolicy, Policy\n",
    "from curiosity.experience.memory import ReplayBuffer\n",
    "from curiosity.experience.util import build_replay_buffer\n",
    "from curiosity.util.util import *\n",
    "from curiosity.rl.qt_opt import cross_entropy_method\n",
    "\n",
    "from cats import CatsExperiment\n",
    "from evaluation import entropy_memory\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "COLLECTION_STEPS = 4096\n",
    "MAX_EPISODE_STEPS = 400\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "#ENV = \"Pendulum-v1\"\n",
    "\n",
    "cfg = DictConfig({\n",
    "    \"env\": {\n",
    "        \"name\": ENV,\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"type\": \"experience_replay\",\n",
    "    #    \"type\": \"prioritized_experience_replay\",\n",
    "    #    \"alpha\": 0.6,\n",
    "    #    \"epsilon\": 0.1,\n",
    "    #    \"beta_0\": 0.4,\n",
    "    #    \"beta_annealing_steps\": COLLECTION_STEPS,\n",
    "        \"normalise_observation\": True\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"initial_collection_size\": 256,\n",
    "        \"total_frames\": COLLECTION_STEPS,\n",
    "        \"minibatch_size\": 128\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"type\": \"qt_opt\",\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"lr\": 0.01,\n",
    "        \"update_frequency\": 1,\n",
    "        \"clip_grad_norm\": 1,\n",
    "        \"ensemble_number\": 5,\n",
    "        \"actor\": {\n",
    "            \"features\": 128\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"features\": 128\n",
    "        }\n",
    "    },\n",
    "    \"intrinsic\": {\n",
    "        \"type\": \"rnd\",\n",
    "        \"encoding_size\": 32,\n",
    "        \"lr\": 0.0003,\n",
    "        \"int_coef\": 1, \n",
    "        \"ext_coef\": 2,\n",
    "        \"reward_normalisation\": True,\n",
    "        \"normalised_obs_clip\": 5\n",
    "    },\n",
    "    \"noise\": {\n",
    "        \"scale\": 0.1,\n",
    "        \"beta\": 0\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = CatsExperiment(\n",
    "    cfg=cfg,\n",
    "    collection_steps=COLLECTION_STEPS,\n",
    "    device=DEVICE,\n",
    "    enable_policy_sampling=True,\n",
    "    teleport_strategy=(\"e_greedy\", 0.1),\n",
    "    seed=7\n",
    ")\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(entropy_memory(experiment.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = CatsExperiment(\n",
    "    cfg=cfg,\n",
    "    collection_steps=COLLECTION_STEPS,\n",
    "    device=DEVICE,\n",
    "    enable_policy_sampling=True,\n",
    "    teleport_strategy=(\"thompson\", 2),\n",
    "    seed=7\n",
    ")\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(entropy_memory(experiment.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = CatsExperiment(\n",
    "    cfg=cfg,\n",
    "    collection_steps=COLLECTION_STEPS,\n",
    "    device=DEVICE,\n",
    "    enable_policy_sampling=True,\n",
    "    teleport_strategy=None,\n",
    "    seed=7\n",
    ")\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(entropy_memory(experiment.memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping Intrinsic Motivation\n",
    "\n",
    "Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "baseline_statistics = {\n",
    "    'total_intrinsic_reward': [],\n",
    "    'newly_collected_intrinsic_reward': [],\n",
    "    'entropy': []\n",
    "}\n",
    "bootstrapping_statistics = copy.deepcopy(baseline_statistics)\n",
    "\n",
    "# Baselne\n",
    "for seed in range(0, N):\n",
    "    experiment = CatsExperiment(\n",
    "        cfg=cfg,\n",
    "        collection_steps=COLLECTION_STEPS,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        enable_policy_sampling=False\n",
    "    )\n",
    "    experiment.run()\n",
    "    for k,v in experiment.log.items():\n",
    "        baseline_statistics[k].append(v)\n",
    "    baseline_statistics['entropy'].append(entropy_memory(experiment.memory))\n",
    "\n",
    "for seed in range(0, N):\n",
    "    experiment = CatsExperiment(\n",
    "        cfg=cfg,\n",
    "        collection_steps=COLLECTION_STEPS,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        enable_policy_sampling=True\n",
    "    )\n",
    "    experiment.run()\n",
    "    for k,v in experiment.log.items():\n",
    "        bootstrapping_statistics[k].append(v)\n",
    "    bootstrapping_statistics['entropy'].append(entropy_memory(experiment.memory))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline\")\n",
    "print(f\"Total Intrinsic Reward: {sum(baseline_statistics['total_intrinsic_reward'])/N}\")\n",
    "print(f\"Collected Intrinsic Reward: {sum(baseline_statistics['newly_collected_intrinsic_reward'])/N}\")\n",
    "print(f\"Entropy: {sum(baseline_statistics['entropy'])/N}\")\n",
    "print(\"Bootstrapped\")\n",
    "print(f\"Total Intrinsic Reward: {sum(bootstrapping_statistics['total_intrinsic_reward'])/N}\")\n",
    "print(f\"Collected Intrinsic Reward: {sum(bootstrapping_statistics['newly_collected_intrinsic_reward'])/N}\")\n",
    "print(f\"Entropy: {sum(bootstrapping_statistics['entropy'])/N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualise_memory(env: gym.Env, *memories: ReplayBuffer):\n",
    "    \"\"\" Visualise state space for given environmentss\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"State Space Coverage\")\n",
    "\n",
    "    if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "        ax.set_xlim(env.observation_space.low[0], env.observation_space.high[0])\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylim(env.observation_space.low[1], env.observation_space.high[1])\n",
    "        ax.set_ylabel(\"Velocity\")\n",
    "    elif env.spec.id == \"Pendulum-v1\":\n",
    "        ax.set_xlim(-math.pi, math.pi)\n",
    "        ax.set_xlabel(\"Theta\")\n",
    "        ax.set_ylim(env.observation_space.low[2], env.observation_space.high[2])\n",
    "        ax.set_ylabel(\"Angular Velocity\")\n",
    "\n",
    "    for memory, name in memories:\n",
    "        batch = Transition(*memory.storage)\n",
    "        s = batch.s_0.cpu().numpy()\n",
    "\n",
    "        # Colors based on time\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=len(s)-1)\n",
    "        cmap = cm.viridis\n",
    "        m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "        colors = m.to_rgba(np.linspace(0, len(s)-1, len(s)))\n",
    "\n",
    "        if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "            ax.scatter(s[:, 0], s[:, 1], s=1, label=name, c=colors)\n",
    "        elif env.spec.id == \"Pendulum-v1\":\n",
    "            ax.scatter(np.arctan2(s[:, 1], s[:, 0]), s[:, 2], s=1, label=name, c=colors)\n",
    "\n",
    "    \n",
    "    fig.colorbar(m, ax=ax)\n",
    "    ax.legend()\n",
    "\n",
    "# Visualise V\n",
    "# For mountain car\n",
    "\n",
    "def visualise_experiment_v(experiment):\n",
    "\n",
    "    X = torch.linspace(experiment.env.observation_space.low[0], experiment.env.observation_space.high[0], 100)\n",
    "    Y = torch.linspace(experiment.env.observation_space.low[1], experiment.env.observation_space.high[1], 100)\n",
    "    grid_X, grid_Y = torch.meshgrid((X, Y))\n",
    "\n",
    "    # Time Aware\n",
    "    #states = torch.stack((grid_X.flatten(), grid_Y.flatten(), 1+torch.zeros_like(grid_X.flatten())) ).T\n",
    "\n",
    "    fig, axs = plt.subplots(1,3)\n",
    "    fig.set_size_inches(20,5)\n",
    "    ax = axs[0]\n",
    "    states = torch.stack((grid_X.flatten(), grid_Y.flatten())).T\n",
    "    states = states.to(DEVICE)\n",
    "        # Observation Normalisation\n",
    "    states_cpu = states.cpu()\n",
    "    states = (states - experiment.policy.normalise_obs.mean) / experiment.policy.normalise_obs.std\n",
    "\n",
    "    # V \n",
    "    values = experiment.V(states)\n",
    "    norm = mpl.colors.Normalize(vmin=values.min(), vmax=values.max())\n",
    "    cmap = cm.viridis\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    colors = m.to_rgba(values.detach().cpu())\n",
    "\n",
    "    ax.set_title(\"Value Function Visualisation\")\n",
    "    ax.scatter(states_cpu[:,0], states_cpu[:, 1], c=colors)\n",
    "    fig.colorbar(m, ax=ax)\n",
    "\n",
    "    # Policy\n",
    "\n",
    "    # Train new actor\n",
    "    #\n",
    "    # new_actor = build_actor(experiment.env, cfg.algorithm.actor.features).to(DEVICE)\n",
    "    # optim_actor = torch.optim.Adam(params=new_actor.net.parameters(), lr=0.1)\n",
    "    # X = torch.linspace(experiment.env.observation_space.low[0], experiment.env.observation_space.high[0], 10)\n",
    "    # Y = torch.linspace(experiment.env.observation_space.low[1], experiment.env.observation_space.high[1], 10)\n",
    "    # grid_X, grid_Y = torch.meshgrid((X, Y))\n",
    "    # pseudo_states = torch.stack((grid_X.flatten(), grid_Y.flatten())).to(DEVICE)\n",
    "    # pseudo_states = pseudo_states.T\n",
    "    # pseudo_states = (pseudo_states - experiment.policy.normalise_obs.mean) / experiment.policy.normalise_obs.std\n",
    "# \n",
    "    # for _ in range(200):\n",
    "    #     # batch, aux = experiment.memory.sample(16)\n",
    "    #     # batch = Transition(*batch)\n",
    "    #     # s_0 = batch.s_0\n",
    "    #     s_0 = pseudo_states\n",
    "    #     desired_action = new_actor(s_0)\n",
    "    #     loss = -experiment.algorithm.critic(torch.cat((s_0, desired_action), 1))\n",
    "    #     loss = torch.mean(torch.mean(loss, dim=list(range(1, len(loss.shape)))))\n",
    "    #     optim_actor.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     torch.nn.utils.clip_grad_norm_(new_actor.net.parameters(), 1)\n",
    "    #     optim_actor.step()\n",
    "# \n",
    "    #     # Evaluate differences between old and new actor\n",
    "    # batch, aux = experiment.memory.sample(1024)\n",
    "    # print(\"Experiment policy value\", experiment.V(batch[0]).mean().detach().item())\n",
    "# \n",
    "    # target_action = new_actor(batch[0])\n",
    "    # V = experiment.algorithm.critic.target(torch.cat((batch[0], target_action), 1))\n",
    "    # print(\"Retrained policy value \", V.mean().detach().item())\n",
    "# \n",
    "\n",
    "\n",
    "    ax = axs[1]\n",
    "    actions = experiment.algorithm.policy_fn(states)\n",
    "    #actions = new_actor(states)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = cm.viridis\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    colors = m.to_rgba(actions.detach().cpu())\n",
    "    ax.scatter(states_cpu[:,0], states_cpu[:, 1], c=colors)\n",
    "    ax.set_title(\"Policy Actions\")\n",
    "    fig.colorbar(m, ax=ax)\n",
    "\n",
    "    # Q Function Optimal\n",
    "    ax = axs[2]\n",
    "\n",
    "    batch_size = 1000\n",
    "    actions = []\n",
    "    for i in range(100*100 // batch_size): \n",
    "        batch = states[i*batch_size : (i+1)*batch_size]\n",
    "        a=cross_entropy_method(\n",
    "                batch,\n",
    "                experiment.algorithm.critic,\n",
    "                experiment.collector.env.action_space,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        actions.append(a)\n",
    "\n",
    "    actions = torch.cat(actions)\n",
    "    print(actions.shape)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = cm.viridis\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    colors = m.to_rgba(actions.detach().cpu())\n",
    "    ax.scatter(states_cpu[:,0], states_cpu[:, 1], c=colors)\n",
    "    ax.set_title(\"Cross Entropy Maximal Actions\")\n",
    "    fig.colorbar(m, ax=ax)\n",
    "\n",
    "    print(f\"Minimum Value {values.min()} | Maximum Value {values.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic Only\n",
    "\n",
    "    # Build RL Structure\n",
    "experiment = Experiment(\n",
    "    cfg,\n",
    "    max_episode_steps=MAX_EPISODE_STEPS,\n",
    "    death_is_not_the_end=True,\n",
    "    fixed_reset=True\n",
    ")\n",
    "experiment.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory(\n",
    "     experiment.env,\n",
    "    (experiment.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_memory(\n",
    "     experiment.env,\n",
    "    (experiment.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(experiment)\n",
    "print(\"Entropy: \", entropy_memory(experiment.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(experiment)\n",
    "print(\"Entropy: \", entropy_memory(experiment.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATS(Experiment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = False,\n",
    "                 epsilon: float = 0.2):\n",
    "        super().__init__(cfg, max_episode_steps, death_is_not_the_end, fixed_reset)\n",
    "\n",
    "        # Recently explored trajectory\n",
    "        self.trajectory = torch.zeros((self.env.spec.max_episode_steps, *self.env.observation_space.shape), device=DEVICE)\n",
    "        # Current time step\n",
    "        self.trajectory_index = 0\n",
    "        # Target Timestep\n",
    "        self.teleport_index = 0\n",
    "        # Reset epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Environment deepcopies\n",
    "        self.state = None\n",
    "        self.quicksaves = [None for _ in range(self.env.spec.max_episode_steps)]\n",
    "\n",
    "        # RNG\n",
    "        self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "\n",
    "        self.log[\"teleport_targets\"] = []\n",
    "        self.log[\"teleport_targets_observations\"] = []\n",
    "        self.log[\"latest_trajectory\"] = []\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        super()._build_env(max_episode_steps)\n",
    "        \n",
    "        # Meta-RL \n",
    "\n",
    "            # Wrap Time Aware Observations\n",
    "            # This adds complexity to the environments, hard to learn?\n",
    "        # self.env = TimeAwareObservation(self.env)\n",
    "        #    # Adjust observation space limit\n",
    "        # igh = self.env.observation_space.high\n",
    "        # igh[-1] = self.env.spec.max_episode_steps\n",
    "        # elf.env.observation_space = gym.spaces.Box(\n",
    "        #    self.env.observation_space.low,\n",
    "        #    high,\n",
    "\n",
    "            # Learn to Truncate\n",
    "        # self.env = self.env.env\n",
    "\n",
    "    def _teleport_selection(self, V):\n",
    "        V = V**2\n",
    "            # Argmax\n",
    "        #teleport_index = torch.argmax(V).item()\n",
    "            # Probability Matching\n",
    "        p = V / V.sum()\n",
    "        pt = self.np_rng.random()\n",
    "        pc = 0\n",
    "        for i, pi in enumerate(p):\n",
    "            pc += pi\n",
    "            if pc >= pt or i == len(p) - 1:\n",
    "                teleport_index = i\n",
    "                break\n",
    "            # TODO: Upper Confidence bound\n",
    "        return teleport_index\n",
    "        \n",
    "    def _reset(self, V):\n",
    "        if self.fixed_reset:\n",
    "            return\n",
    "            # Epsilon Greedy Reset\n",
    "        #if torch.rand(1) < epsilon:\n",
    "        #    obs, infos = collector.env.reset()\n",
    "        #    resets[0] = collector.env\n",
    "        #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "        #    teleport_index = 0\n",
    "            # Reset Buffer\n",
    "        reset_buffer = []\n",
    "        reset_buffer_obs = []\n",
    "        for i in range(10):\n",
    "            obs, info = self.collector.env.reset(seed=int(self.np_rng.integers(65536)))\n",
    "            reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "            reset_buffer_obs.append(obs)\n",
    "        reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs, dtype=np.float32), device=DEVICE)\n",
    "        V_r = self.V(reset_buffer_obs)\n",
    "        best_reset_index = torch.argmax(V_r).item()\n",
    "        #if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "        if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "            self.collector.env = reset_buffer[best_reset_index]\n",
    "            self.quicksaves[0] = self.collector.env\n",
    "            self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "            self.teleport_index = 0\n",
    "\n",
    "    def run(self):\n",
    "        self.early_start(cfg.train.initial_collection_size)\n",
    "        self.env_reset()\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        # Main Loop\n",
    "        self.state = copy.deepcopy(self.collector.env)\n",
    "        self.newly_collected_intrinsic_reward = []\n",
    "        for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            self.newly_collected_intrinsic_reward.append(self.intrinsic.reward(Transition(\n",
    "                torch.tensor(obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(action, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(reward, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(n_obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(terminated, device=DEVICE).unsqueeze(0)\n",
    "            ))[2].item())\n",
    "\n",
    "            # Time Limit Normalisation\n",
    "            # obs, n_obs = copy.deepcopy(obs), copy.deepcopy(n_obs)\n",
    "            # obs[-1] = obs[-1] / self.env.spec.max_episode_steps\n",
    "            # n_obs[-1] = n_obs[-1] / self.env.spec.max_episode_steps\n",
    "\n",
    "            # Update trajectory\n",
    "            self.quicksaves[self.trajectory_index] = self.state\n",
    "            self.trajectory[self.trajectory_index] = torch.from_numpy(obs).to(DEVICE)\n",
    "            self.state = copy.deepcopy(self.collector.env)\n",
    "            self.trajectory_index += 1\n",
    "\n",
    "            # Manage Teleportation\n",
    "            if truncated or terminated:\n",
    "                # Calculate Value\n",
    "                V = self.V(self.trajectory[:self.trajectory_index])\n",
    "                # Teleportation Selection\n",
    "                with torch.no_grad():\n",
    "                    self.teleport_index = self._teleport_selection(V)\n",
    "                #print(self.teleport_index)\n",
    "                # Resets\n",
    "                self._reset(V)\n",
    "\n",
    "                # Teleport\n",
    "                self.collector.env = self.quicksaves[self.teleport_index]\n",
    "                self.collector.obs = self.trajectory[self.teleport_index].cpu().numpy()\n",
    "                self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "                self.trajectory_index = self.teleport_index\n",
    "                self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "                # Log\n",
    "                self.log[\"teleport_targets\"].append(self.teleport_index)\n",
    "                self.log[\"teleport_targets_observations\"].append(self.collector.obs)\n",
    "                self.log[\"latest_trajectory\"] = self.trajectory.cpu().numpy()\n",
    "\n",
    "                # Account for time\n",
    "                # terminated = True\n",
    "                # n_obs = self.collector.obs\n",
    "\n",
    "\n",
    "            # Update Memory\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "            \n",
    "            # Remaining RL Update\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "\n",
    "\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            r_i = r_i\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.algorithm.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = CATS(\n",
    "    cfg,\n",
    "    max_episode_steps=MAX_EPISODE_STEPS,\n",
    "    death_is_not_the_end=True,\n",
    "    fixed_reset=True\n",
    ")\n",
    "cats.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory(\n",
    "      cats.env,\n",
    "     (cats.memory, \"teleport\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    cats.log['latest_trajectory'][:,0],\n",
    "    cats.log['latest_trajectory'][:,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cats.log['teleport_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(cats)\n",
    "print(\"Entropy: \", entropy_memory(cats.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn To Truncate\n",
    "\n",
    "class CATS_Truncate(Experiment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = False,\n",
    "                 storage_size: float = 100,\n",
    "                 epsilon: float = 0.1):\n",
    "        super().__init__(cfg, max_episode_steps, death_is_not_the_end, fixed_reset)\n",
    "\n",
    "        self.storage_size = storage_size\n",
    "        # Recently explored trajectory\n",
    "        self.trajectory = []\n",
    "\n",
    "        # Current time step\n",
    "        self.trajectory_index = 0\n",
    "        self.trajectory_counter = 0\n",
    "        # Target Timestep\n",
    "        self.teleport_index = 0\n",
    "        # Reset epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Environment deepcopies\n",
    "        self.state = None\n",
    "        self.quicksaves = []\n",
    "\n",
    "        # RNG\n",
    "        self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "\n",
    "        self.log[\"teleport_targets\"] = []\n",
    "        self.log[\"teleport_targets_observations\"] = []\n",
    "        self.log[\"latest_trajectory\"] = []\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        super()._build_env(max_episode_steps)\n",
    "\n",
    "        # Learn to Truncate\n",
    "        self.env = self.env.env\n",
    "\n",
    "    def _teleport_selection(self, V):\n",
    "        #V = V**2\n",
    "            # Argmax\n",
    "        teleport_index = torch.argmax(V).item()\n",
    "            # Probability Matching\n",
    "        #p = V / V.sum()\n",
    "        # pt = self.np_rng.random()\n",
    "        # pc = 0\n",
    "        # for i, pi in enumerate(p):\n",
    "        #     pc += pi\n",
    "        #     if pc >= pt or i == len(p) - 1:\n",
    "        #         teleport_index = i\n",
    "        #         break\n",
    "        #     # TODO: Upper Confidence bound\n",
    "        return teleport_index\n",
    "\n",
    "    def _reset(self, V):\n",
    "        if self.fixed_reset:\n",
    "            return\n",
    "            # Epsilon Greedy Reset\n",
    "        #if torch.rand(1) < epsilon:\n",
    "        #    obs, infos = collector.env.reset()\n",
    "        #    resets[0] = collector.env\n",
    "        #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "        #    teleport_index = 0\n",
    "            # Reset Buffer\n",
    "        reset_buffer = []\n",
    "        reset_buffer_obs = []\n",
    "        for i in range(10):\n",
    "            obs, info = self.collector.env.reset(seed=int(self.np_rng.integers(65536)))\n",
    "            reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "            reset_buffer_obs.append(obs)\n",
    "        reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs, dtype=np.float32), device=DEVICE)\n",
    "        V_r = self.V(reset_buffer_obs)\n",
    "        best_reset_index = torch.argmax(V_r).item()\n",
    "        #if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "        if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "            self.collector.env = reset_buffer[best_reset_index]\n",
    "            self.quicksaves[0] = self.collector.env\n",
    "            self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "            self.teleport_index = 0\n",
    "\n",
    "    def run(self):\n",
    "        if self.fixed_reset:\n",
    "            self.collector.env.reset(seed=self.cfg.seed)\n",
    "\n",
    "        early_start_transitions = self.collector.early_start(cfg.train.initial_collection_size)\n",
    "        for t in early_start_transitions:\n",
    "            self._update_memory(*t)\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        # Main Loop\n",
    "        if self.fixed_reset:\n",
    "            self.collector.env.reset(seed=self.cfg.seed)\n",
    "        else:\n",
    "            self.collector.env.reset()\n",
    "\n",
    "        self.state = copy.deepcopy(self.collector.env)\n",
    "        newly_collected_intrinsic_reward = 0\n",
    "        for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "            obs, action, reward, n_obs, terminated, _ = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            newly_collected_intrinsic_reward += self.intrinsic.reward(Transition(\n",
    "                torch.tensor(obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(action, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(reward, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(n_obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(terminated, device=DEVICE).unsqueeze(0)\n",
    "            ))[2].item()\n",
    "\n",
    "            if self.trajectory_counter == self.storage_size - 1:\n",
    "                truncated = True\n",
    "            else:\n",
    "                truncated = False\n",
    "            # Time Limit Normalisation\n",
    "            # obs, n_obs = copy.deepcopy(obs), copy.deepcopy(n_obs)\n",
    "            # obs[-1] = obs[-1] / self.env.spec.max_episode_steps\n",
    "            # n_obs[-1] = n_obs[-1] / self.env.spec.max_episode_steps\n",
    "\n",
    "            # Update trajectory\n",
    "            self.quicksaves.append(self.state)\n",
    "            self.trajectory.append(obs)\n",
    "            self.state = copy.deepcopy(self.collector.env)\n",
    "            self.trajectory_index += 1\n",
    "            self.trajectory_counter += 1\n",
    "\n",
    "            # Manage Teleportation\n",
    "            if truncated or terminated:\n",
    "                # Calculate Value\n",
    "                V = self.V(torch.tensor(self.trajectory,device=DEVICE))\n",
    "                # Teleportation Selection\n",
    "                with torch.no_grad():\n",
    "                    self.teleport_index = self._teleport_selection(V)\n",
    "                # Resets\n",
    "                self._reset(V)\n",
    "\n",
    "                # Teleport\n",
    "                self.collector.env = self.quicksaves[self.teleport_index]\n",
    "                self.collector.obs = self.trajectory[self.teleport_index]\n",
    "                self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "\n",
    "                self.trajectory_index = self.teleport_index\n",
    "                self.trajectory_counter = 0\n",
    "                self.trajectory = self.trajectory[:self.teleport_index+1]\n",
    "                self.quicksaves = self.quicksaves[:self.teleport_index+1]\n",
    "                self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "                # Log\n",
    "                self.log[\"teleport_targets\"].append(self.teleport_index)\n",
    "                self.log[\"teleport_targets_observations\"].append(self.collector.obs)\n",
    "                self.log[\"latest_trajectory\"] = self.trajectory\n",
    "\n",
    "            # Update Memory\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "            \n",
    "            # Remaining RL Update\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "\n",
    "\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.algorithm.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n",
    "\n",
    "        print(newly_collected_intrinsic_reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_truncate = CATS_Truncate(\n",
    "    cfg,\n",
    "    max_episode_steps=9999,\n",
    "    death_is_not_the_end=True,\n",
    "    storage_size=400,\n",
    "    fixed_reset=True\n",
    ")\n",
    "cats_truncate.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory(\n",
    "      cats_truncate.env,\n",
    "     (cats_truncate.memory, \"teleport\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(cats_truncate)\n",
    "print(\"Entropy: \", entropy_memory(cats_truncate.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cats.log['total_intrinsic_reward'])\n",
    "print(experiment.log['total_intrinsic_reward'])\n",
    "print(cats_truncate.log['total_intrinsic_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(experiment.newly_collected_intrinsic_reward[MAX_EPISODE_STEPS:], label=\"baseline\")\n",
    "ax.plot(cats.newly_collected_intrinsic_reward[MAX_EPISODE_STEPS:], label=\"cats\")\n",
    "ax.plot(cats_truncate.newly_collected_intrinsic_reward[MAX_EPISODE_STEPS:], label=\"cats_truncate\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
