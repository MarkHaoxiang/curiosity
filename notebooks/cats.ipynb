{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration\n",
    "\n",
    "This notebook logs exploratory results on adding teleportation on MCC with state coverage visualisation\n",
    "\n",
    "21/01/2024\n",
    "- Naive teleportation to argmax works\n",
    "- Longer episodes are better than shorter\n",
    "- Different intrinsic rewards show significantly different behavior\n",
    "- Even naively, general improvement over pure intrinsic\n",
    "- Fails to beat intrinsic + extrinsic: perhaps this is due to negative extrinsic reward revealing data on target? Not comparable, and I think fully explored in that reward shifting paper\n",
    "- Keeps teleporting to same target\n",
    "- This may be a problem with DDPG\n",
    "\n",
    "\n",
    "28/01/2024\n",
    "- Probabilistic teleportation work well\n",
    "- Environment reset stochasticity is important\n",
    "- Time limit aware Q functions are difficult to train!\n",
    "- Proposal: Dynamic Truncation!\n",
    "\n",
    "4/02/2024\n",
    "- ICM and RND leads to inherently different results - RND should be prioritised\n",
    "- CATS fails to improve over baseline on RND with fixed reset, but does in ICM. After reset, the new trajectory follows the previous trajectory too closely, while resetting from the start leads to more divergence across the entire episode (and hence more exploration)\n",
    "- Fixing the reset states leads to improved analysis\n",
    "- Policy function gets stuck in the local minima of the Q function\n",
    "- Analyse DQN instead? Skip parametrized policy function and use an approximator?? Maybe implement QT-opt https://arxiv.org/pdf/1806.10293.pdf. This may be important to obtain interesting experiment results, since on MCC the policy generally fails to follow the critic even on large learning rates (why??)\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Confidence Bounds (How? Without latent density estimator?)\n",
    "- Termination as an action\n",
    "- Epsilon greedy\n",
    "- Time aware exploration\n",
    "\n",
    "Known Failure Modes\n",
    "- Teleporting to the end of the episode, and immediately truncating\n",
    "- \n",
    "\n",
    "Ideas\n",
    "- Bootstrapped Q value estimate for confidence bound guided estimation?\n",
    "\n",
    "Interesting observations\n",
    "- Qt_opt directly on critic, rather than target network explores faster??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Imports and shared training information\n",
    "\n",
    "# Std\n",
    "import copy\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# Training\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from omegaconf import DictConfig\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeAwareObservation\n",
    "\n",
    "# Evaluation\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Curiosity\n",
    "from curiosity.experience import Transition\n",
    "from curiosity.experience.collector import GymCollector\n",
    "from curiosity.policy import ColoredNoisePolicy, Policy\n",
    "from curiosity.experience.memory import ReplayBuffer\n",
    "from curiosity.experience.util import build_replay_buffer\n",
    "from curiosity.util.util import *\n",
    "from curiosity.rl.qt_opt import cross_entropy_method, QTOpt\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "COLLECTION_STEPS = 4096\n",
    "MAX_EPISODE_STEPS = 400\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "#ENV = \"Pendulum-v1\"\n",
    "\n",
    "cfg = DictConfig({\n",
    "    \"seed\": 6230,\n",
    "    \"env\": {\n",
    "        \"name\": ENV,\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"type\": \"experience_replay\",\n",
    "    #    \"type\": \"prioritized_experience_replay\",\n",
    "    #    \"alpha\": 0.6,\n",
    "    #    \"epsilon\": 0.1,\n",
    "    #    \"beta_0\": 0.4,\n",
    "    #    \"beta_annealing_steps\": COLLECTION_STEPS,\n",
    "        \"normalise_observation\": True\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"initial_collection_size\": 256,\n",
    "        \"total_frames\": COLLECTION_STEPS,\n",
    "        \"minibatch_size\": 128\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"type\": \"qt_opt\",\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"lr\": 0.01,\n",
    "        \"update_frequency\": 1,\n",
    "        \"clip_grad_norm\": 1,\n",
    "        \"ensemble_number\": 5,\n",
    "        \"actor\": {\n",
    "            \"features\": 128\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"features\": 128\n",
    "        }\n",
    "    },\n",
    "    \"intrinsic\": {\n",
    "        \"type\": \"rnd\",\n",
    "        \"encoding_size\": 32,\n",
    "        \"lr\": 0.0003,\n",
    "        \"int_coef\": 1, \n",
    "        \"ext_coef\": 2,\n",
    "        \"reward_normalisation\": True,\n",
    "        \"normalised_obs_clip\": 5\n",
    "    },\n",
    "    \"noise\": {\n",
    "        \"scale\": 0.1,\n",
    "        \"beta\": 0\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, List, Tuple\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "from curiosity.rl import Algorithm\n",
    "from curiosity.rl.qt_opt import cross_entropy_method\n",
    "from curiosity.experience import AuxiliaryMemoryData\n",
    "from curiosity.nn import Critic, AddTargetNetwork\n",
    "\n",
    "class QTOptCats(Algorithm):\n",
    "    \"\"\" Q-learning for continuous actions with Cross-Entropy Maximisation\n",
    "\n",
    "    With ensemble uncertainty estimation and dual critic\n",
    "\n",
    "    Kalashnikov et al. QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 build_critic: Callable[[], Critic],\n",
    "                 obs_space: gym.spaces.Box,\n",
    "                 action_space: gym.spaces.Box,\n",
    "                 ensemble_number: int = 5,\n",
    "                 gamma: float = 0.99,\n",
    "                 lr: float = 1e-3,\n",
    "                 tau: float = 0.005,\n",
    "                 cem_n: int = 64,\n",
    "                 cem_m: int = 6,\n",
    "                 cem_n_iterations: int = 2,\n",
    "                 clip_grad_norm: Optional[float] = 1,\n",
    "                 update_frequency: int = 1,\n",
    "                 device: str = \"cpu\",\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.critics = [AddTargetNetwork(build_critic(), device=device) for _ in range(ensemble_number)]\n",
    "\n",
    "        self.device=device\n",
    "\n",
    "        self._ensemble_number = ensemble_number\n",
    "        self._gamma = gamma\n",
    "        self._tau = tau\n",
    "        self._env_action_scale = torch.tensor(action_space.high-action_space.low, device=device) / 2.0\n",
    "        self._env_action_min = torch.tensor(action_space.low, dtype=torch.float32, device=device)\n",
    "        self._env_action_max = torch.tensor(action_space.low, dtype=torch.float32, device=device)\n",
    "        self._obs_space = obs_space\n",
    "        self._action_space = action_space\n",
    "        self._clip_grad_norm = clip_grad_norm\n",
    "        self._update_frequency = update_frequency\n",
    "        self._n = cem_n\n",
    "        self._m = cem_m\n",
    "        self._n_iterations = cem_n_iterations\n",
    "        self._chosen_critic = 0\n",
    "\n",
    "        self._optim_critic = torch.optim.Adam(\n",
    "            params=torch.nn.ModuleList(self.critics).parameters(),\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "        self.loss_critic_value = 0\n",
    "    \n",
    "    @property\n",
    "    def critic(self):\n",
    "        return self.critics[self._chosen_critic]\n",
    "\n",
    "    def policy_fn(self, s: Union[Tensor, np.ndarray], critic: Optional[Critic] = None) -> Tensor:\n",
    "        if isinstance(s, np.ndarray):\n",
    "            s = torch.tensor(s, device=self.device)\n",
    "        if critic is None:\n",
    "            critic = self.critic\n",
    "        squeeze = False\n",
    "        if self._obs_space.shape == s.shape:\n",
    "            squeeze = True\n",
    "            s = s.unsqueeze(0)\n",
    "        result = cross_entropy_method(\n",
    "            s_0=s,\n",
    "            critic_network=critic,\n",
    "            action_space=self._action_space,\n",
    "            n=self._n,\n",
    "            m=self._m,\n",
    "            n_iterations=self._n_iterations,\n",
    "            device=self.device\n",
    "        )\n",
    "        if squeeze:\n",
    "            result = result.squeeze()\n",
    "        return result\n",
    "\n",
    "    def reset_critic(self):\n",
    "        self._chosen_critic = random.randint(0,self._ensemble_number-1)\n",
    "\n",
    "    def _critic_update(self, batch: Transition, aux: AuxiliaryMemoryData):\n",
    "        x = [critic.q(batch.s_0, batch.a).squeeze() for critic in self.critics]        \n",
    "        with torch.no_grad():\n",
    "            # Implement a variant of Clipped Double-Q Learning\n",
    "            # Randomly sample two networks\n",
    "            sampled_critics = random.sample(self.critics, 2)\n",
    "            a_1 = self.policy_fn(batch.s_1, critic=sampled_critics[0].target)\n",
    "            a_2 = self.policy_fn(batch.s_1, critic=sampled_critics[1].target)\n",
    "            target_max_1 = sampled_critics[0].target.q(batch.s_1, a_1).squeeze()\n",
    "            target_max_2 = sampled_critics[1].target.q(batch.s_1, a_2).squeeze()\n",
    "            y = batch.r + (~batch.d) * torch.minimum(target_max_1, target_max_2) * self._gamma\n",
    "\n",
    "        losses = []\n",
    "        for x_i in x:\n",
    "            losses.append(torch.mean((aux.weights * (y-x_i)) ** 2))\n",
    "        loss_critic = sum(losses)\n",
    "\n",
    "        loss_value = loss_critic.item()\n",
    "        self._optim_critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        if not self._clip_grad_norm is None:\n",
    "            for critic in self.critics:\n",
    "                nn.utils.clip_grad_norm_(critic.net.parameters(), self._clip_grad_norm)\n",
    "        self._optim_critic.step()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def update(self, batch: Transition, aux: AuxiliaryMemoryData, step: int):\n",
    "        if step % self._update_frequency == 0:\n",
    "            self.loss_critic_value =  self._critic_update(batch, aux)\n",
    "            for critic in self.critics:\n",
    "                critic.update_target_network(tau = self._tau)\n",
    "        return self.loss_critic_value\n",
    "\n",
    "    def get_log(self):\n",
    "        return {\n",
    "            \"critic_loss\": self.loss_critic_value,\n",
    "        }\n",
    "\n",
    "    def get_models(self) -> List[Tuple[nn.Module, str]]:\n",
    "        return list(zip(self.critics, [f\"critic_{i}\" for i in range(len(self.critics))]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_memory(memory: ReplayBuffer):\n",
    "    # Construct a density estimator\n",
    "    s = Transition(*memory.sample(len(memory))[0]).s_0.cpu().numpy()\n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=\"scott\").fit(s)\n",
    "    log_likelihoods = kde.score_samples(kde.sample(n_samples=10000))\n",
    "    return -log_likelihoods.mean()\n",
    "\n",
    "def visualise_memory(env: gym.Env, *memories: ReplayBuffer):\n",
    "    \"\"\" Visualise state space for given environmentss\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"State Space Coverage\")\n",
    "\n",
    "    if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "        ax.set_xlim(env.observation_space.low[0], env.observation_space.high[0])\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylim(env.observation_space.low[1], env.observation_space.high[1])\n",
    "        ax.set_ylabel(\"Velocity\")\n",
    "    elif env.spec.id == \"Pendulum-v1\":\n",
    "        ax.set_xlim(-math.pi, math.pi)\n",
    "        ax.set_xlabel(\"Theta\")\n",
    "        ax.set_ylim(env.observation_space.low[2], env.observation_space.high[2])\n",
    "        ax.set_ylabel(\"Angular Velocity\")\n",
    "\n",
    "    for memory, name in memories:\n",
    "        batch = Transition(*memory.storage)\n",
    "        s = batch.s_0.cpu().numpy()\n",
    "\n",
    "        # Colors based on time\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=len(s)-1)\n",
    "        cmap = cm.viridis\n",
    "        m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "        colors = m.to_rgba(np.linspace(0, len(s)-1, len(s)))\n",
    "\n",
    "        if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "            ax.scatter(s[:, 0], s[:, 1], s=1, label=name, c=colors)\n",
    "        elif env.spec.id == \"Pendulum-v1\":\n",
    "            ax.scatter(np.arctan2(s[:, 1], s[:, 0]), s[:, 2], s=1, label=name, c=colors)\n",
    "\n",
    "    \n",
    "    fig.colorbar(m, ax=ax)\n",
    "    ax.legend()\n",
    "\n",
    "# Visualise V\n",
    "# For mountain car\n",
    "\n",
    "def visualise_experiment_v(experiment):\n",
    "\n",
    "    X = torch.linspace(experiment.env.observation_space.low[0], experiment.env.observation_space.high[0], 100)\n",
    "    Y = torch.linspace(experiment.env.observation_space.low[1], experiment.env.observation_space.high[1], 100)\n",
    "    grid_X, grid_Y = torch.meshgrid((X, Y))\n",
    "\n",
    "    # Time Aware\n",
    "    #states = torch.stack((grid_X.flatten(), grid_Y.flatten(), 1+torch.zeros_like(grid_X.flatten())) ).T\n",
    "\n",
    "    fig, axs = plt.subplots(1,3)\n",
    "    fig.set_size_inches(20,5)\n",
    "    ax = axs[0]\n",
    "    states = torch.stack((grid_X.flatten(), grid_Y.flatten())).T\n",
    "    states = states.to(DEVICE)\n",
    "        # Observation Normalisation\n",
    "    states_cpu = states.cpu()\n",
    "    states = (states - experiment.policy.normalise_obs.mean) / experiment.policy.normalise_obs.std\n",
    "\n",
    "    # V \n",
    "    values = experiment.V(states)\n",
    "    norm = mpl.colors.Normalize(vmin=values.min(), vmax=values.max())\n",
    "    cmap = cm.viridis\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    colors = m.to_rgba(values.detach().cpu())\n",
    "\n",
    "    ax.set_title(\"Value Function Visualisation\")\n",
    "    ax.scatter(states_cpu[:,0], states_cpu[:, 1], c=colors)\n",
    "    fig.colorbar(m, ax=ax)\n",
    "\n",
    "    # Policy\n",
    "\n",
    "    # Train new actor\n",
    "    #\n",
    "    # new_actor = build_actor(experiment.env, cfg.algorithm.actor.features).to(DEVICE)\n",
    "    # optim_actor = torch.optim.Adam(params=new_actor.net.parameters(), lr=0.1)\n",
    "    # X = torch.linspace(experiment.env.observation_space.low[0], experiment.env.observation_space.high[0], 10)\n",
    "    # Y = torch.linspace(experiment.env.observation_space.low[1], experiment.env.observation_space.high[1], 10)\n",
    "    # grid_X, grid_Y = torch.meshgrid((X, Y))\n",
    "    # pseudo_states = torch.stack((grid_X.flatten(), grid_Y.flatten())).to(DEVICE)\n",
    "    # pseudo_states = pseudo_states.T\n",
    "    # pseudo_states = (pseudo_states - experiment.policy.normalise_obs.mean) / experiment.policy.normalise_obs.std\n",
    "# \n",
    "    # for _ in range(200):\n",
    "    #     # batch, aux = experiment.memory.sample(16)\n",
    "    #     # batch = Transition(*batch)\n",
    "    #     # s_0 = batch.s_0\n",
    "    #     s_0 = pseudo_states\n",
    "    #     desired_action = new_actor(s_0)\n",
    "    #     loss = -experiment.algorithm.critic(torch.cat((s_0, desired_action), 1))\n",
    "    #     loss = torch.mean(torch.mean(loss, dim=list(range(1, len(loss.shape)))))\n",
    "    #     optim_actor.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     torch.nn.utils.clip_grad_norm_(new_actor.net.parameters(), 1)\n",
    "    #     optim_actor.step()\n",
    "# \n",
    "    #     # Evaluate differences between old and new actor\n",
    "    # batch, aux = experiment.memory.sample(1024)\n",
    "    # print(\"Experiment policy value\", experiment.V(batch[0]).mean().detach().item())\n",
    "# \n",
    "    # target_action = new_actor(batch[0])\n",
    "    # V = experiment.algorithm.critic.target(torch.cat((batch[0], target_action), 1))\n",
    "    # print(\"Retrained policy value \", V.mean().detach().item())\n",
    "# \n",
    "\n",
    "\n",
    "    ax = axs[1]\n",
    "    actions = experiment.algorithm.policy_fn(states)\n",
    "    #actions = new_actor(states)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = cm.viridis\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    colors = m.to_rgba(actions.detach().cpu())\n",
    "    ax.scatter(states_cpu[:,0], states_cpu[:, 1], c=colors)\n",
    "    ax.set_title(\"Policy Actions\")\n",
    "    fig.colorbar(m, ax=ax)\n",
    "\n",
    "    # Q Function Optimal\n",
    "    ax = axs[2]\n",
    "\n",
    "    batch_size = 1000\n",
    "    actions = []\n",
    "    for i in range(100*100 // batch_size): \n",
    "        batch = states[i*batch_size : (i+1)*batch_size]\n",
    "        a=cross_entropy_method(\n",
    "                batch,\n",
    "                experiment.algorithm.critic,\n",
    "                experiment.collector.env.action_space,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        actions.append(a)\n",
    "\n",
    "    actions = torch.cat(actions)\n",
    "    print(actions.shape)\n",
    "    norm = mpl.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = cm.viridis\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    colors = m.to_rgba(actions.detach().cpu())\n",
    "    ax.scatter(states_cpu[:,0], states_cpu[:, 1], c=colors)\n",
    "    ax.set_title(\"Cross Entropy Maximal Actions\")\n",
    "    fig.colorbar(m, ax=ax)\n",
    "\n",
    "    print(f\"Minimum Value {values.min()} | Maximum Value {values.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\" Baseline control class for intrinsic exploration experiment\n",
    "\n",
    "    Core assumptions\n",
    "    - DDPG (Or similar offline )\n",
    "\n",
    "    This notebook primarily explores the concept of \"teleportation\"\n",
    "    And the edits involve data collection\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = True):\n",
    "        self.cfg = copy.deepcopy(cfg)\n",
    "        self.death_is_not_the_end = death_is_not_the_end\n",
    "        self.fixed_reset = fixed_reset\n",
    "\n",
    "        self._build_env(max_episode_steps)\n",
    "        self._build_policy()\n",
    "        self._build_data()\n",
    "        self.intrinsic = build_intrinsic(self.env, self.cfg.intrinsic, device=DEVICE)\n",
    "\n",
    "        self.log = {\n",
    "            \"total_intrinsic_reward\": 0 # An ideal exploration algorithm maximises the recieved intrinsic reward\n",
    "        }\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        self.env = gym.make(\n",
    "            self.cfg.env.name,\n",
    "            render_mode=\"rgb_array\",\n",
    "            max_episode_steps=max_episode_steps\n",
    "        )\n",
    "        self.env.reset()\n",
    "        self.rng = global_seed(self.cfg.seed, self.env)\n",
    "\n",
    "    def _build_policy(self):\n",
    "        self.algorithm = QTOptCats(\n",
    "            build_critic=lambda : build_critic(self.env, **self.cfg.algorithm.critic).to(DEVICE),\n",
    "            action_space=self.env.action_space,\n",
    "            obs_space=self.env.observation_space,\n",
    "            device=DEVICE,\n",
    "            **self.cfg.algorithm\n",
    "        )\n",
    "        self.policy = ColoredNoisePolicy(\n",
    "            self.algorithm.policy_fn,\n",
    "            self.env.action_space,\n",
    "            self.env.spec.max_episode_steps,\n",
    "            rng=self.rng,\n",
    "            device=DEVICE,\n",
    "            **self.cfg.noise\n",
    "        )\n",
    "\n",
    "    def _build_data(self):\n",
    "        self.memory = build_replay_buffer(self.env,capacity=COLLECTION_STEPS, device=DEVICE, normalise_observation=True)\n",
    "            # Remove automatic memory addition for more control\n",
    "        self.collector = GymCollector(self.policy, self.env, device=DEVICE)\n",
    "        self.policy.normalise_obs = self.memory.rmv[0]\n",
    "\n",
    "    def _update_memory(self, obs, action, reward, n_obs, terminated, truncated):\n",
    "        self.memory.append((obs, action, reward, n_obs, terminated))\n",
    "\n",
    "    def V(self, s) -> torch.Tensor:\n",
    "        \"\"\"Calculates the value function for states s\n",
    "        \"\"\"\n",
    "        target_action = self.algorithm.policy_fn(s)\n",
    "        V = self.algorithm.critic.target.q(s, target_action)\n",
    "        return V\n",
    "    \n",
    "    def env_reset(self):\n",
    "        #self.algorithm.reset_critic()\n",
    "        if self.fixed_reset:\n",
    "            o, i = self.collector.env.reset(seed=self.cfg.seed)\n",
    "        else:\n",
    "            o, i = self.collector.env.reset()\n",
    "        self.collector.obs = o\n",
    "\n",
    "    def early_start(self, n: int):\n",
    "        \"\"\" Overrides early start for tighter control\n",
    "\n",
    "        Args:\n",
    "            n (int): number of steps\n",
    "        \"\"\"\n",
    "        self.env_reset()    \n",
    "        policy = self.collector.policy\n",
    "        self.collector.set_policy(Policy(lambda _: self.env.action_space.sample(), transform_obs=False))\n",
    "        for i in range(n):\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1, early_start=True)[-1]\n",
    "            if terminated or truncated:\n",
    "                self.env_reset()\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "        self.collector.policy = policy\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Default Experiment run\n",
    "        \"\"\"\n",
    "        self.early_start(cfg.train.initial_collection_size)\n",
    "        self.env_reset()\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        self.newly_collected_intrinsic_reward = []\n",
    "\n",
    "        for step in tqdm(range(1, self.cfg.train.total_frames+1)):\n",
    "            # Collect Data\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            self.newly_collected_intrinsic_reward.append(self.intrinsic.reward(Transition(\n",
    "                torch.tensor(obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(action, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(reward, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(n_obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(terminated, device=DEVICE).unsqueeze(0)\n",
    "            ))[2].item())\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                self.env_reset() \n",
    "\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)                \n",
    "\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "            # Intrinsic Reward Calculation\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            r_i = r_i\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "            # RL Update            \n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.algorithm.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic Only\n",
    "\n",
    "    # Build RL Structure\n",
    "baseline = Experiment(\n",
    "    cfg,\n",
    "    max_episode_steps=MAX_EPISODE_STEPS,\n",
    "    death_is_not_the_end=True,\n",
    "    fixed_reset=True\n",
    ")\n",
    "baseline.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory(\n",
    "     baseline.env,\n",
    "    (baseline.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_memory(\n",
    "     baseline.env,\n",
    "    (baseline.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(baseline)\n",
    "print(\"Entropy: \", entropy_memory(baseline.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(baseline)\n",
    "print(\"Entropy: \", entropy_memory(baseline.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATS(Experiment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = False,\n",
    "                 epsilon: float = 0.2):\n",
    "        super().__init__(cfg, max_episode_steps, death_is_not_the_end, fixed_reset)\n",
    "\n",
    "        # Recently explored trajectory\n",
    "        self.trajectory = torch.zeros((self.env.spec.max_episode_steps, *self.env.observation_space.shape), device=DEVICE)\n",
    "        # Current time step\n",
    "        self.trajectory_index = 0\n",
    "        # Target Timestep\n",
    "        self.teleport_index = 0\n",
    "        # Reset epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Environment deepcopies\n",
    "        self.state = None\n",
    "        self.quicksaves = [None for _ in range(self.env.spec.max_episode_steps)]\n",
    "\n",
    "        # RNG\n",
    "        self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "\n",
    "        self.log[\"teleport_targets\"] = []\n",
    "        self.log[\"teleport_targets_observations\"] = []\n",
    "        self.log[\"latest_trajectory\"] = []\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        super()._build_env(max_episode_steps)\n",
    "        \n",
    "        # Meta-RL \n",
    "\n",
    "            # Wrap Time Aware Observations\n",
    "            # This adds complexity to the environments, hard to learn?\n",
    "        # self.env = TimeAwareObservation(self.env)\n",
    "        #    # Adjust observation space limit\n",
    "        # igh = self.env.observation_space.high\n",
    "        # igh[-1] = self.env.spec.max_episode_steps\n",
    "        # elf.env.observation_space = gym.spaces.Box(\n",
    "        #    self.env.observation_space.low,\n",
    "        #    high,\n",
    "\n",
    "            # Learn to Truncate\n",
    "        # self.env = self.env.env\n",
    "\n",
    "    def _teleport_selection(self, V):\n",
    "        V = V**2\n",
    "            # Argmax\n",
    "        #teleport_index = torch.argmax(V).item()\n",
    "            # Probability Matching\n",
    "        p = V / V.sum()\n",
    "        pt = self.np_rng.random()\n",
    "        pc = 0\n",
    "        for i, pi in enumerate(p):\n",
    "            pc += pi\n",
    "            if pc >= pt or i == len(p) - 1:\n",
    "                teleport_index = i\n",
    "                break\n",
    "            # TODO: Upper Confidence bound\n",
    "        return teleport_index\n",
    "        \n",
    "    def _reset(self, V):\n",
    "        if self.fixed_reset:\n",
    "            return\n",
    "            # Epsilon Greedy Reset\n",
    "        #if torch.rand(1) < epsilon:\n",
    "        #    obs, infos = collector.env.reset()\n",
    "        #    resets[0] = collector.env\n",
    "        #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "        #    teleport_index = 0\n",
    "            # Reset Buffer\n",
    "        reset_buffer = []\n",
    "        reset_buffer_obs = []\n",
    "        for i in range(10):\n",
    "            obs, info = self.collector.env.reset(seed=int(self.np_rng.integers(65536)))\n",
    "            reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "            reset_buffer_obs.append(obs)\n",
    "        reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs, dtype=np.float32), device=DEVICE)\n",
    "        V_r = self.V(reset_buffer_obs)\n",
    "        best_reset_index = torch.argmax(V_r).item()\n",
    "        #if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "        if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "            self.collector.env = reset_buffer[best_reset_index]\n",
    "            self.quicksaves[0] = self.collector.env\n",
    "            self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "            self.teleport_index = 0\n",
    "\n",
    "    def run(self):\n",
    "        self.early_start(cfg.train.initial_collection_size)\n",
    "        self.env_reset()\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        # Main Loop\n",
    "        self.state = copy.deepcopy(self.collector.env)\n",
    "        self.newly_collected_intrinsic_reward = []\n",
    "        for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            self.newly_collected_intrinsic_reward.append(self.intrinsic.reward(Transition(\n",
    "                torch.tensor(obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(action, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(reward, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(n_obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(terminated, device=DEVICE).unsqueeze(0)\n",
    "            ))[2].item())\n",
    "\n",
    "            # Time Limit Normalisation\n",
    "            # obs, n_obs = copy.deepcopy(obs), copy.deepcopy(n_obs)\n",
    "            # obs[-1] = obs[-1] / self.env.spec.max_episode_steps\n",
    "            # n_obs[-1] = n_obs[-1] / self.env.spec.max_episode_steps\n",
    "\n",
    "            # Update trajectory\n",
    "            self.quicksaves[self.trajectory_index] = self.state\n",
    "            self.trajectory[self.trajectory_index] = torch.from_numpy(obs).to(DEVICE)\n",
    "            self.state = copy.deepcopy(self.collector.env)\n",
    "            self.trajectory_index += 1\n",
    "\n",
    "            # Manage Teleportation\n",
    "            if truncated or terminated:\n",
    "                # Calculate Value\n",
    "                V = self.V(self.trajectory[:self.trajectory_index])\n",
    "                # Teleportation Selection\n",
    "                with torch.no_grad():\n",
    "                    self.teleport_index = self._teleport_selection(V)\n",
    "                #print(self.teleport_index)\n",
    "                # Resets\n",
    "                self._reset(V)\n",
    "\n",
    "                # Teleport\n",
    "                self.collector.env = self.quicksaves[self.teleport_index]\n",
    "                self.collector.obs = self.trajectory[self.teleport_index].cpu().numpy()\n",
    "                self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "                self.trajectory_index = self.teleport_index\n",
    "                self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "                # Log\n",
    "                self.log[\"teleport_targets\"].append(self.teleport_index)\n",
    "                self.log[\"teleport_targets_observations\"].append(self.collector.obs)\n",
    "                self.log[\"latest_trajectory\"] = self.trajectory.cpu().numpy()\n",
    "\n",
    "                # Account for time\n",
    "                # terminated = True\n",
    "                # n_obs = self.collector.obs\n",
    "\n",
    "\n",
    "            # Update Memory\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "            \n",
    "            # Remaining RL Update\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "\n",
    "\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            r_i = r_i\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.algorithm.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = CATS(\n",
    "    cfg,\n",
    "    max_episode_steps=MAX_EPISODE_STEPS,\n",
    "    death_is_not_the_end=True,\n",
    "    fixed_reset=True\n",
    ")\n",
    "cats.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory(\n",
    "      cats.env,\n",
    "     (cats.memory, \"teleport\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    cats.log['latest_trajectory'][:,0],\n",
    "    cats.log['latest_trajectory'][:,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cats.log['teleport_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(cats)\n",
    "print(\"Entropy: \", entropy_memory(cats.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn To Truncate\n",
    "\n",
    "class CATS_Truncate(Experiment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = False,\n",
    "                 storage_size: float = 100,\n",
    "                 epsilon: float = 0.1):\n",
    "        super().__init__(cfg, max_episode_steps, death_is_not_the_end, fixed_reset)\n",
    "\n",
    "        self.storage_size = storage_size\n",
    "        # Recently explored trajectory\n",
    "        self.trajectory = []\n",
    "\n",
    "        # Current time step\n",
    "        self.trajectory_index = 0\n",
    "        self.trajectory_counter = 0\n",
    "        # Target Timestep\n",
    "        self.teleport_index = 0\n",
    "        # Reset epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Environment deepcopies\n",
    "        self.state = None\n",
    "        self.quicksaves = []\n",
    "\n",
    "        # RNG\n",
    "        self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "\n",
    "        self.log[\"teleport_targets\"] = []\n",
    "        self.log[\"teleport_targets_observations\"] = []\n",
    "        self.log[\"latest_trajectory\"] = []\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        super()._build_env(max_episode_steps)\n",
    "\n",
    "        # Learn to Truncate\n",
    "        self.env = self.env.env\n",
    "\n",
    "    def _teleport_selection(self, V):\n",
    "        #V = V**2\n",
    "            # Argmax\n",
    "        teleport_index = torch.argmax(V).item()\n",
    "            # Probability Matching\n",
    "        #p = V / V.sum()\n",
    "        # pt = self.np_rng.random()\n",
    "        # pc = 0\n",
    "        # for i, pi in enumerate(p):\n",
    "        #     pc += pi\n",
    "        #     if pc >= pt or i == len(p) - 1:\n",
    "        #         teleport_index = i\n",
    "        #         break\n",
    "        #     # TODO: Upper Confidence bound\n",
    "        return teleport_index\n",
    "\n",
    "    def _reset(self, V):\n",
    "        if self.fixed_reset:\n",
    "            return\n",
    "            # Epsilon Greedy Reset\n",
    "        #if torch.rand(1) < epsilon:\n",
    "        #    obs, infos = collector.env.reset()\n",
    "        #    resets[0] = collector.env\n",
    "        #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "        #    teleport_index = 0\n",
    "            # Reset Buffer\n",
    "        reset_buffer = []\n",
    "        reset_buffer_obs = []\n",
    "        for i in range(10):\n",
    "            obs, info = self.collector.env.reset(seed=int(self.np_rng.integers(65536)))\n",
    "            reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "            reset_buffer_obs.append(obs)\n",
    "        reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs, dtype=np.float32), device=DEVICE)\n",
    "        V_r = self.V(reset_buffer_obs)\n",
    "        best_reset_index = torch.argmax(V_r).item()\n",
    "        #if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "        if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "            self.collector.env = reset_buffer[best_reset_index]\n",
    "            self.quicksaves[0] = self.collector.env\n",
    "            self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "            self.teleport_index = 0\n",
    "\n",
    "    def run(self):\n",
    "        if self.fixed_reset:\n",
    "            self.collector.env.reset(seed=self.cfg.seed)\n",
    "\n",
    "        early_start_transitions = self.collector.early_start(cfg.train.initial_collection_size)\n",
    "        for t in early_start_transitions:\n",
    "            self._update_memory(*t)\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        # Main Loop\n",
    "        if self.fixed_reset:\n",
    "            self.collector.env.reset(seed=self.cfg.seed)\n",
    "        else:\n",
    "            self.collector.env.reset()\n",
    "\n",
    "        self.state = copy.deepcopy(self.collector.env)\n",
    "        newly_collected_intrinsic_reward = 0\n",
    "        for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "            obs, action, reward, n_obs, terminated, _ = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            newly_collected_intrinsic_reward += self.intrinsic.reward(Transition(\n",
    "                torch.tensor(obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(action, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(reward, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(n_obs, device=DEVICE).unsqueeze(0),\n",
    "                torch.tensor(terminated, device=DEVICE).unsqueeze(0)\n",
    "            ))[2].item()\n",
    "\n",
    "            if self.trajectory_counter == self.storage_size - 1:\n",
    "                truncated = True\n",
    "            else:\n",
    "                truncated = False\n",
    "            # Time Limit Normalisation\n",
    "            # obs, n_obs = copy.deepcopy(obs), copy.deepcopy(n_obs)\n",
    "            # obs[-1] = obs[-1] / self.env.spec.max_episode_steps\n",
    "            # n_obs[-1] = n_obs[-1] / self.env.spec.max_episode_steps\n",
    "\n",
    "            # Update trajectory\n",
    "            self.quicksaves.append(self.state)\n",
    "            self.trajectory.append(obs)\n",
    "            self.state = copy.deepcopy(self.collector.env)\n",
    "            self.trajectory_index += 1\n",
    "            self.trajectory_counter += 1\n",
    "\n",
    "            # Manage Teleportation\n",
    "            if truncated or terminated:\n",
    "                # Calculate Value\n",
    "                V = self.V(torch.tensor(self.trajectory,device=DEVICE))\n",
    "                # Teleportation Selection\n",
    "                with torch.no_grad():\n",
    "                    self.teleport_index = self._teleport_selection(V)\n",
    "                # Resets\n",
    "                self._reset(V)\n",
    "\n",
    "                # Teleport\n",
    "                self.collector.env = self.quicksaves[self.teleport_index]\n",
    "                self.collector.obs = self.trajectory[self.teleport_index]\n",
    "                self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "\n",
    "                self.trajectory_index = self.teleport_index\n",
    "                self.trajectory_counter = 0\n",
    "                self.trajectory = self.trajectory[:self.teleport_index+1]\n",
    "                self.quicksaves = self.quicksaves[:self.teleport_index+1]\n",
    "                self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "                # Log\n",
    "                self.log[\"teleport_targets\"].append(self.teleport_index)\n",
    "                self.log[\"teleport_targets_observations\"].append(self.collector.obs)\n",
    "                self.log[\"latest_trajectory\"] = self.trajectory\n",
    "\n",
    "            # Update Memory\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "            \n",
    "            # Remaining RL Update\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "\n",
    "\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.algorithm.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n",
    "\n",
    "        print(newly_collected_intrinsic_reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_truncate = CATS_Truncate(\n",
    "    cfg,\n",
    "    max_episode_steps=9999,\n",
    "    death_is_not_the_end=True,\n",
    "    storage_size=400,\n",
    "    fixed_reset=True\n",
    ")\n",
    "cats_truncate.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory(\n",
    "      cats_truncate.env,\n",
    "     (cats_truncate.memory, \"teleport\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_experiment_v(cats_truncate)\n",
    "print(\"Entropy: \", entropy_memory(cats_truncate.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cats.log['total_intrinsic_reward'])\n",
    "print(baseline.log['total_intrinsic_reward'])\n",
    "print(cats_truncate.log['total_intrinsic_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(baseline.newly_collected_intrinsic_reward[MAX_EPISODE_STEPS:], label=\"baseline\")\n",
    "ax.plot(cats.newly_collected_intrinsic_reward[MAX_EPISODE_STEPS:], label=\"cats\")\n",
    "ax.plot(cats_truncate.newly_collected_intrinsic_reward[MAX_EPISODE_STEPS:], label=\"cats_truncate\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
