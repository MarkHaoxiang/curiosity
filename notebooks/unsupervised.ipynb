{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration\n",
    "\n",
    "This notebook logs exploratory results on adding teleportation on MCC with state coverage visualisation\n",
    "\n",
    "21/01/2024\n",
    "- Naive teleportation to argmax works\n",
    "- Longer episodes are better than shorter\n",
    "- Different intrinsic rewards show significantly different behavior\n",
    "- Even naively, general improvement over pure intrinsic\n",
    "- Fails to beat intrinsic + extrinsic: perhaps this is due to negative extrinsic reward revealing data on target? Not comparable, and I think fully explored in that reward shifting paper\n",
    "- Keeps teleporting to same target\n",
    "- This may be a problem with DDPG\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Confidence Bounds\n",
    "- Termination as an action\n",
    "- Epsilon greedy\n",
    "- Time aware exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Imports and shared training information\n",
    "\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "from curiosity.experience import Transition\n",
    "from curiosity.experience.collector import GymCollector\n",
    "from curiosity.policy import ColoredNoisePolicy\n",
    "from curiosity.experience.memory import ReplayBuffer\n",
    "from curiosity.experience.util import build_replay_buffer, build_collector\n",
    "from curiosity.util.util import global_seed, build_intrinsic, build_rl\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "COLLECTION_STEPS = 20000\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "#ENV = \"Pendulum-v1\"\n",
    "\n",
    "cfg = DictConfig({\n",
    "    \"seed\": 0,\n",
    "    \"env\": {\n",
    "        \"name\": ENV,\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"type\": \"experience_replay\"\n",
    "    #    \"type\": \"prioritized_experience_replay\",\n",
    "    #    \"alpha\": 0.6,\n",
    "    #    \"epsilon\": 0.1,\n",
    "    #    \"beta_0\": 0.4,\n",
    "    #    \"beta_annealing_steps\": COLLECTION_STEPS\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"initial_collection_size\": 500,\n",
    "        \"total_frames\": COLLECTION_STEPS,\n",
    "        \"minibatch_size\": 128   \n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"type\": \"ddpg\",\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"lr\": 0.001,\n",
    "        \"update_frequency\": 1,\n",
    "        \"clip_grad_norm\": 1,\n",
    "        \"actor\": {\n",
    "            \"features\": 128\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"features\": 128\n",
    "        }\n",
    "    },\n",
    "    \"intrinsic\": {\n",
    "        \"type\": \"rnd\",\n",
    "        \"encoding_size\": 32,\n",
    "        \"lr\": 0.0003,\n",
    "        \"int_coef\": 1, \n",
    "        \"ext_coef\": 2,\n",
    "        \"obs_normalisation\": True,\n",
    "        \"reward_normalisation\": True,\n",
    "        \"normalised_obs_clip\": 5\n",
    "    },\n",
    "    \"noise\": {\n",
    "        \"scale\": 0.1,\n",
    "        \"beta\": 0\n",
    "    }\n",
    "})\n",
    "\n",
    "def visualise_memory_mcc(env: gym.Env, *memories: ReplayBuffer):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"State Space Coverage\")\n",
    "\n",
    "    if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "        ax.set_xlim(env.observation_space.low[0], env.observation_space.high[0])\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylim(env.observation_space.low[1], env.observation_space.high[1])\n",
    "        ax.set_ylabel(\"Velocity\")\n",
    "    elif env.spec.id == \"Pendulum-v1\":\n",
    "        ax.set_xlim(-math.pi, math.pi)\n",
    "        ax.set_xlabel(\"Theta\")\n",
    "        ax.set_ylim(env.observation_space.low[2], env.observation_space.high[2])\n",
    "        ax.set_ylabel(\"Angular Velocity\")\n",
    "\n",
    "    for memory, name in memories:\n",
    "        batch = Transition(*memory.storage)\n",
    "        s = batch.s_0.cpu().numpy()\n",
    "\n",
    "        if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "            ax.scatter(s[:, 0], s[:, 1], s=1, label=name)\n",
    "        elif env.spec.id == \"Pendulum-v1\":\n",
    "            ax.scatter(np.arctan2(s[:, 1], s[:, 0]), s[:, 2], s=1, label=name)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\" Baseline control class for intrinsic exploration experiment\n",
    "\n",
    "    Core assumptions\n",
    "    - DDPG (Or similar offline )\n",
    "\n",
    "    This notebook primarily explores the concept of \"teleportation\"\n",
    "    And the edits involve data collection\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, max_episode_steps=None, death_is_not_the_end = True):\n",
    "        self.cfg = copy.deepcopy(cfg)\n",
    "        self.death_is_not_the_end = death_is_not_the_end\n",
    "\n",
    "        self._build_env(max_episode_steps)\n",
    "        self._build_policy()\n",
    "        self._build_data()\n",
    "        self.intrinsic = build_intrinsic(self.env, self.cfg.intrinsic, device=DEVICE)\n",
    "\n",
    "        self.log = {}\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        self.env = gym.make(\n",
    "            self.cfg.env.name,\n",
    "            render_mode=\"rgb_array\",\n",
    "            max_episode_steps=max_episode_steps\n",
    "        )\n",
    "        self.env.reset()\n",
    "        self.rng = global_seed(self.cfg.seed, self.env)\n",
    "\n",
    "    def _build_policy(self):\n",
    "        self.ddpg = build_rl(self.env, self.cfg.algorithm, device=DEVICE)\n",
    "        self.policy = ColoredNoisePolicy(\n",
    "            self.ddpg.actor,\n",
    "            self.env.action_space,\n",
    "            self.env.spec.max_episode_steps,\n",
    "            rng=self.rng,\n",
    "            device=DEVICE,\n",
    "            **self.cfg.noise\n",
    "        )\n",
    "\n",
    "    def _build_data(self):\n",
    "        self.memory = build_replay_buffer(self.env, capacity=COLLECTION_STEPS, device=DEVICE)\n",
    "        self.collector = build_collector(self.policy, self.env, self.memory, device=DEVICE)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Default Experiment run\n",
    "        \"\"\"\n",
    "\n",
    "        self.collector.early_start(cfg.train.initial_collection_size)\n",
    "        batch, aux = self.memory.sample(cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        for step in tqdm(range(1, self.cfg.train.total_frames+1)):\n",
    "            # Collect Data\n",
    "            self.collector.collect(n=1)\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "            # Intrinsic Reward Calculation\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "            # RL Update            \n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.ddpg.update(batch, aux, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic Only\n",
    "\n",
    "    # Build RL Structure\n",
    "baseline = Experiment(cfg, max_episode_steps=300, death_is_not_the_end=False)\n",
    "baseline.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory_mcc(\n",
    "     baseline.env,\n",
    "    (baseline.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATS(Experiment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps=None,\n",
    "                 death_is_not_the_end=True,\n",
    "                 epsilon: float =0.1):\n",
    "        super().__init__(cfg, max_episode_steps, death_is_not_the_end)\n",
    "\n",
    "        # Recently explored trajectory\n",
    "        self.trajectory = torch.zeros((self.env.spec.max_episode_steps, *self.env.observation_space.shape), device=DEVICE)\n",
    "        # Current time step\n",
    "        self.trajectory_index = 0\n",
    "        # Target Timestep\n",
    "        self.teleport_index = 0\n",
    "        # Reset epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Environment deepcopies\n",
    "        self.state = None\n",
    "        self.quicksaves = [None for _ in range(self.env.spec.max_episode_steps)]\n",
    "\n",
    "        # RNG\n",
    "        self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "\n",
    "\n",
    "    def _build_data(self):\n",
    "        self.memory = build_replay_buffer(self.env,capacity=COLLECTION_STEPS, device=DEVICE)\n",
    "            # Remove automatic memory addition for more control\n",
    "        self.collector = GymCollector(self.policy, self.env, device=DEVICE)\n",
    "\n",
    "    def _update_memory(self, obs, action, reward, n_obs, terminated, truncated):\n",
    "        self.memory.append((obs, action, reward, n_obs, terminated))\n",
    "\n",
    "    def _teleport_selection(self, V):\n",
    "        V = V**2\n",
    "            # Argmax\n",
    "        # teleport_index = torch.argmax(V).item()\n",
    "            # Probability Matching\n",
    "        p = V / V.sum()\n",
    "        pt = self.np_rng.random()\n",
    "        pc = 0\n",
    "        for i, pi in enumerate(p):\n",
    "            pc += pi\n",
    "            if pc >= pt or i == len(p) - 1:\n",
    "                teleport_index = i\n",
    "                break\n",
    "            # TODO: Upper Confidence bound\n",
    "        return teleport_index\n",
    "        \n",
    "    def _reset(self, V):\n",
    "            # Epsilon Greedy Reset\n",
    "        #if torch.rand(1) < epsilon:\n",
    "        #    obs, infos = collector.env.reset()\n",
    "        #    resets[0] = collector.env\n",
    "        #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "        #    teleport_index = 0\n",
    "            # Reset Buffer\n",
    "        reset_buffer = []\n",
    "        reset_buffer_obs = []\n",
    "        for i in range(10):\n",
    "            obs, info = self.collector.env.reset()\n",
    "            reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "            reset_buffer_obs.append(obs)\n",
    "        reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs), device=DEVICE)\n",
    "        target_action = self.ddpg.actor.target(reset_buffer_obs)\n",
    "        V_r = self.ddpg.critic.target(torch.cat((reset_buffer_obs, target_action),1))\n",
    "        best_reset_index = torch.argmax(V_r).item()\n",
    "        #if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "        if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "            self.collector.env = reset_buffer[best_reset_index]\n",
    "            self.quicksaves[0] = self.collector.env\n",
    "            self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "            self.teleport_index = 0\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        early_start_transitions = self.collector.early_start(cfg.train.initial_collection_size)\n",
    "        for t in early_start_transitions:\n",
    "            self._update_memory(*t)\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        # Main Loop\n",
    "        self.collector.env.reset()\n",
    "        self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "        for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            # Update trajectory\n",
    "            self.quicksaves[self.trajectory_index] = self.state\n",
    "            self.trajectory[self.trajectory_index] = torch.from_numpy(obs).to(DEVICE)\n",
    "            self.state = copy.deepcopy(self.collector.env)\n",
    "            self.trajectory_index += 1\n",
    "\n",
    "            # Manage Teleportation\n",
    "            if truncated or terminated:\n",
    "                # Calculate Value\n",
    "                t = self.trajectory[:self.trajectory_index]\n",
    "                target_action = self.ddpg.actor.target(t)\n",
    "                V = self.ddpg.critic.target(torch.cat((t, target_action), 1))\n",
    "\n",
    "                # Teleportation Selection\n",
    "                    # Argmax\n",
    "                #teleport_index = torch.argmax(V).item()\n",
    "                    # Probability matching\n",
    "                with torch.no_grad():\n",
    "                    self.teleport_index = self._teleport_selection(V)\n",
    "                \n",
    "                self._reset(V)\n",
    "\n",
    "                # Teleport\n",
    "                self.collector.env = self.quicksaves[self.teleport_index]\n",
    "                self.collector.obs = self.trajectory[self.teleport_index].cpu().numpy()\n",
    "                self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "                self.trajectory_index = self.teleport_index\n",
    "                self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "            \n",
    "            # Update Memory\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "            \n",
    "            # Remaining RL Update\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "\n",
    "\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.ddpg.update(batch, aux, step=step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Build RL Structure\n",
    "cats = CATS(cfg, max_episode_steps=300, death_is_not_the_end=False)\n",
    "cats.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory_mcc(\n",
    "     cats.env,\n",
    "    (cats.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
