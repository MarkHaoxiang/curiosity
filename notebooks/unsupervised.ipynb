{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration\n",
    "\n",
    "This notebook logs exploratory results on adding teleportation on MCC with state coverage visualisation\n",
    "\n",
    "21/01/2024\n",
    "- Naive teleportation to argmax works\n",
    "- Longer episodes are better than shorter\n",
    "- Different intrinsic rewards show significantly different behavior\n",
    "- Even naively, general improvement over pure intrinsic\n",
    "- Fails to beat intrinsic + extrinsic: perhaps this is due to negative extrinsic reward revealing data on target? Not comparable, and I think fully explored in that reward shifting paper\n",
    "- Keeps teleporting to same target\n",
    "- This may be a problem with DDPG\n",
    "\n",
    "\n",
    "28/01/2024\n",
    "- Probabilistic teleportation work well\n",
    "- Environment reset stochasticity is important\n",
    "- Time limit aware Q functions are difficult to train!\n",
    "- Proposal: Dynamic Truncation!\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Confidence Bounds\n",
    "- Termination as an action\n",
    "- Epsilon greedy\n",
    "- Time aware exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Imports and shared training information\n",
    "\n",
    "import copy\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeAwareObservation\n",
    "from curiosity.experience import Transition\n",
    "from curiosity.experience.collector import GymCollector\n",
    "from curiosity.policy import ColoredNoisePolicy\n",
    "from curiosity.experience.memory import ReplayBuffer\n",
    "from curiosity.experience.util import build_replay_buffer\n",
    "from curiosity.util.util import global_seed, build_intrinsic, build_rl\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "COLLECTION_STEPS = 10000\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "#ENV = \"Pendulum-v1\"\n",
    "\n",
    "cfg = DictConfig({\n",
    "    \"seed\": 1,\n",
    "    \"env\": {\n",
    "        \"name\": ENV,\n",
    "    },\n",
    "    \"memory\": {\n",
    "    #    \"type\": \"experience_replay\"\n",
    "        \"type\": \"prioritized_experience_replay\",\n",
    "        \"alpha\": 0.6,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"beta_0\": 0.4,\n",
    "        \"beta_annealing_steps\": COLLECTION_STEPS\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"initial_collection_size\": 500,\n",
    "        \"total_frames\": COLLECTION_STEPS,\n",
    "        \"minibatch_size\": 128   \n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"type\": \"ddpg\",\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"lr\": 0.001,\n",
    "        \"update_frequency\": 1,\n",
    "        \"clip_grad_norm\": 1,\n",
    "        \"actor\": {\n",
    "            \"features\": 128\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"features\": 128\n",
    "        }\n",
    "    },\n",
    "    \"intrinsic\": {\n",
    "        \"type\": \"rnd\",\n",
    "        \"encoding_size\": 32,\n",
    "        \"lr\": 0.0003,\n",
    "        \"int_coef\": 1, \n",
    "        \"ext_coef\": 2,\n",
    "        \"obs_normalisation\": True,\n",
    "        \"reward_normalisation\": True,\n",
    "        \"normalised_obs_clip\": 5\n",
    "    },\n",
    "    \"noise\": {\n",
    "        \"scale\": 0.1,\n",
    "        \"beta\": 0\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_memory_mcc(env: gym.Env, *memories: ReplayBuffer):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"State Space Coverage\")\n",
    "\n",
    "    if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "        ax.set_xlim(env.observation_space.low[0], env.observation_space.high[0])\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylim(env.observation_space.low[1], env.observation_space.high[1])\n",
    "        ax.set_ylabel(\"Velocity\")\n",
    "    elif env.spec.id == \"Pendulum-v1\":\n",
    "        ax.set_xlim(-math.pi, math.pi)\n",
    "        ax.set_xlabel(\"Theta\")\n",
    "        ax.set_ylim(env.observation_space.low[2], env.observation_space.high[2])\n",
    "        ax.set_ylabel(\"Angular Velocity\")\n",
    "\n",
    "    for memory, name in memories:\n",
    "        batch = Transition(*memory.storage)\n",
    "        s = batch.s_0.cpu().numpy()\n",
    "\n",
    "        # Colors based on time\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=len(s)-1)\n",
    "        cmap = cm.viridis\n",
    "        m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "        colors = m.to_rgba(np.linspace(0, len(s)-1, len(s)))\n",
    "\n",
    "\n",
    "        if env.spec.id == \"MountainCarContinuous-v0\":\n",
    "            ax.scatter(s[:, 0], s[:, 1], s=1, label=name, c=colors)\n",
    "        elif env.spec.id == \"Pendulum-v1\":\n",
    "            ax.scatter(np.arctan2(s[:, 1], s[:, 0]), s[:, 2], s=1, label=name, c=colors)\n",
    "\n",
    "    \n",
    "    fig.colorbar(m, ax=ax)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \"\"\" Baseline control class for intrinsic exploration experiment\n",
    "\n",
    "    Core assumptions\n",
    "    - DDPG (Or similar offline )\n",
    "\n",
    "    This notebook primarily explores the concept of \"teleportation\"\n",
    "    And the edits involve data collection\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = True):\n",
    "        self.cfg = copy.deepcopy(cfg)\n",
    "        self.death_is_not_the_end = death_is_not_the_end\n",
    "        self.fixed_reset = fixed_reset\n",
    "\n",
    "        self._build_env(max_episode_steps)\n",
    "        self._build_policy()\n",
    "        self._build_data()\n",
    "        self.intrinsic = build_intrinsic(self.env, self.cfg.intrinsic, device=DEVICE)\n",
    "\n",
    "        self.log = {\n",
    "            \"total_intrinsic_reward\": 0 # An ideal exploration algorithm maximises the recieved intrinsic reward\n",
    "        }\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        self.env = gym.make(\n",
    "            self.cfg.env.name,\n",
    "            render_mode=\"rgb_array\",\n",
    "            max_episode_steps=max_episode_steps\n",
    "        )\n",
    "        self.env.reset()\n",
    "        self.rng = global_seed(self.cfg.seed, self.env)\n",
    "\n",
    "    def _build_policy(self):\n",
    "        self.ddpg = build_rl(self.env, self.cfg.algorithm, device=DEVICE)\n",
    "        self.policy = ColoredNoisePolicy(\n",
    "            self.ddpg.actor,\n",
    "            self.env.action_space,\n",
    "            self.env.spec.max_episode_steps,\n",
    "            rng=self.rng,\n",
    "            device=DEVICE,\n",
    "            **self.cfg.noise\n",
    "        )\n",
    "\n",
    "    def _build_data(self):\n",
    "        self.memory = build_replay_buffer(self.env,capacity=COLLECTION_STEPS, device=DEVICE)\n",
    "            # Remove automatic memory addition for more control\n",
    "        self.collector = GymCollector(self.policy, self.env, device=DEVICE)\n",
    "\n",
    "    def _update_memory(self, obs, action, reward, n_obs, terminated, truncated):\n",
    "        self.memory.append((obs, action, reward, n_obs, terminated))\n",
    "\n",
    "    def early_start(self):\n",
    "        early_start_transitions = self.collector.early_start(cfg.train.initial_collection_size)\n",
    "        for t in early_start_transitions:\n",
    "            self._update_memory(*t)\n",
    "    \n",
    "    def V(self, s) -> torch.Tensor:\n",
    "        \"\"\"Calculates the value function for states s\n",
    "        \"\"\"\n",
    "        target_action = self.ddpg.actor.target(s)\n",
    "        V = self.ddpg.critic.target(torch.cat((s, target_action), 1))\n",
    "        return V\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Default Experiment run\n",
    "        \"\"\"\n",
    "        self.early_start()\n",
    "        \n",
    "        batch, aux = self.memory.sample(cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        for step in tqdm(range(1, self.cfg.train.total_frames+1)):\n",
    "            # Collect Data\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1)[-1]\n",
    "            if (terminated or truncated) and self.fixed_reset:\n",
    "                self.collector.env.reset(seed=self.cfg.seed)\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)                \n",
    "\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "            # Intrinsic Reward Calculation\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "            # RL Update            \n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.ddpg.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic Only\n",
    "\n",
    "    # Build RL Structure\n",
    "baseline = Experiment(\n",
    "    cfg,\n",
    "    max_episode_steps=300,\n",
    "    death_is_not_the_end=True,\n",
    "    fixed_reset=False\n",
    ")\n",
    "baseline.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory_mcc(\n",
    "     baseline.env,\n",
    "    (baseline.memory, \"intrinsic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATS(Experiment):\n",
    "\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 max_episode_steps: Optional[int] = None,\n",
    "                 death_is_not_the_end: bool = True,\n",
    "                 fixed_reset: bool = False,\n",
    "                 epsilon: float = 0.2):\n",
    "        super().__init__(cfg, max_episode_steps, death_is_not_the_end, fixed_reset)\n",
    "\n",
    "        # Recently explored trajectory\n",
    "        self.trajectory = torch.zeros((self.env.spec.max_episode_steps, *self.env.observation_space.shape), device=DEVICE)\n",
    "        # Current time step\n",
    "        self.trajectory_index = 0\n",
    "        # Target Timestep\n",
    "        self.teleport_index = 0\n",
    "        # Reset epsilon\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Environment deepcopies\n",
    "        self.state = None\n",
    "        self.quicksaves = [None for _ in range(self.env.spec.max_episode_steps)]\n",
    "\n",
    "        # RNG\n",
    "        self.np_rng = np.random.default_rng(self.cfg.seed)\n",
    "\n",
    "        self.log[\"teleport_targets\"] = []\n",
    "        self.log[\"teleport_targets_observations\"] = []\n",
    "        self.log[\"latest_trajectory\"] = []\n",
    "\n",
    "    def _build_env(self, max_episode_steps=None):\n",
    "        super()._build_env(max_episode_steps)\n",
    "        \n",
    "        # Meta-RL \n",
    "\n",
    "            # Wrap Time Aware Observations\n",
    "            # This adds complexity to the environments, hard to learn?\n",
    "        # self.env = TimeAwareObservation(self.env)\n",
    "        #    # Adjust observation space limit\n",
    "        # igh = self.env.observation_space.high\n",
    "        # igh[-1] = self.env.spec.max_episode_steps\n",
    "        # elf.env.observation_space = gym.spaces.Box(\n",
    "        #    self.env.observation_space.low,\n",
    "        #    high,\n",
    "\n",
    "            # Learn to Truncate\n",
    "        # self.env = self.env.env\n",
    "\n",
    "    def _teleport_selection(self, V):\n",
    "        V = V**2\n",
    "            # Argmax\n",
    "        # teleport_index = torch.argmax(V).item()\n",
    "            # Probability Matching\n",
    "        p = V / V.sum()\n",
    "        pt = self.np_rng.random()\n",
    "        pc = 0\n",
    "        for i, pi in enumerate(p):\n",
    "            pc += pi\n",
    "            if pc >= pt or i == len(p) - 1:\n",
    "                teleport_index = i\n",
    "                break\n",
    "            # TODO: Upper Confidence bound\n",
    "        return teleport_index\n",
    "        \n",
    "    def _reset(self, V):\n",
    "        if self.fixed_reset:\n",
    "            return\n",
    "            # Epsilon Greedy Reset\n",
    "        #if torch.rand(1) < epsilon:\n",
    "        #    obs, infos = collector.env.reset()\n",
    "        #    resets[0] = collector.env\n",
    "        #    trajectory[0] = torch.tensor(obs, device=DEVICE)\n",
    "        #    teleport_index = 0\n",
    "            # Reset Buffer\n",
    "        reset_buffer = []\n",
    "        reset_buffer_obs = []\n",
    "        for i in range(10):\n",
    "            obs, info = self.collector.env.reset(seed=int(self.np_rng.integers(65536)))\n",
    "            reset_buffer.append(copy.deepcopy(self.collector.env))\n",
    "            reset_buffer_obs.append(obs)\n",
    "        reset_buffer_obs = torch.tensor(np.array(reset_buffer_obs, dtype=np.float32), device=DEVICE)\n",
    "        V_r = self.V(reset_buffer_obs)\n",
    "        best_reset_index = torch.argmax(V_r).item()\n",
    "        if self.np_rng.random() < self.epsilon or V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "        #if V_r[best_reset_index] >= V[self.teleport_index]:\n",
    "            self.collector.env = reset_buffer[best_reset_index]\n",
    "            self.quicksaves[0] = self.collector.env\n",
    "            self.trajectory[0] = reset_buffer_obs[best_reset_index]\n",
    "            self.teleport_index = 0\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        early_start_transitions = self.collector.early_start(cfg.train.initial_collection_size)\n",
    "        for t in early_start_transitions:\n",
    "            self._update_memory(*t)\n",
    "\n",
    "        batch, aux = self.memory.sample(self.cfg.train.initial_collection_size)\n",
    "        self.intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "        # Main Loop\n",
    "        if self.fixed_reset:\n",
    "            self.collector.env.reset(seed=self.cfg.seed)\n",
    "        else:\n",
    "            self.collector.env.reset()\n",
    "        self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "        for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "            obs, action, reward, n_obs, terminated, truncated = self.collector.collect(n=1)[-1]\n",
    "\n",
    "            # Time Limit Normalisation\n",
    "            # obs, n_obs = copy.deepcopy(obs), copy.deepcopy(n_obs)\n",
    "            # obs[-1] = obs[-1] / self.env.spec.max_episode_steps\n",
    "            # n_obs[-1] = n_obs[-1] / self.env.spec.max_episode_steps\n",
    "\n",
    "            # Update trajectory\n",
    "            self.quicksaves[self.trajectory_index] = self.state\n",
    "            self.trajectory[self.trajectory_index] = torch.from_numpy(obs).to(DEVICE)\n",
    "            self.state = copy.deepcopy(self.collector.env)\n",
    "            self.trajectory_index += 1\n",
    "\n",
    "            # Manage Teleportation\n",
    "            if truncated or terminated:\n",
    "                # Calculate Value\n",
    "                V = self.V(self.trajectory[:self.trajectory_index])\n",
    "                # Teleportation Selection\n",
    "                with torch.no_grad():\n",
    "                    self.teleport_index = self._teleport_selection(V)\n",
    "                # Resets\n",
    "                self._reset(V)\n",
    "\n",
    "                # Teleport\n",
    "                self.collector.env = self.quicksaves[self.teleport_index]\n",
    "                self.collector.obs = self.trajectory[self.teleport_index].cpu().numpy()\n",
    "                self.collector.env.np_random = np.random.default_rng(self.np_rng.integers(65536))\n",
    "                self.trajectory_index = self.teleport_index\n",
    "                self.state = copy.deepcopy(self.collector.env)\n",
    "\n",
    "                # Log\n",
    "                self.log[\"teleport_targets\"].append(self.teleport_index)\n",
    "                self.log[\"teleport_targets_observations\"].append(self.collector.obs)\n",
    "                self.log[\"latest_trajectory\"] = self.trajectory.cpu().numpy()\n",
    "\n",
    "                # Account for time\n",
    "                # terminated = True\n",
    "                # n_obs = self.collector.obs\n",
    "\n",
    "\n",
    "            # Update Memory\n",
    "            self._update_memory(obs, action, reward, n_obs, terminated, truncated)\n",
    "            \n",
    "            # Remaining RL Update\n",
    "            batch, aux = self.memory.sample(self.cfg.train.minibatch_size)\n",
    "            batch = Transition(*batch)\n",
    "            if self.death_is_not_the_end:\n",
    "                batch = Transition(batch.s_0, batch.a, batch.r, batch.s_1, torch.zeros(batch.d.shape, device=DEVICE).bool())\n",
    "\n",
    "\n",
    "            r_t, r_e, r_i = self.intrinsic.reward(batch)\n",
    "            self.intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "            batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "            self.ddpg.update(batch, aux, step=step)\n",
    "\n",
    "            # Log\n",
    "            self.log[\"total_intrinsic_reward\"] += r_i.mean().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Build RL Structure\n",
    "cats = CATS(\n",
    "    cfg,\n",
    "    max_episode_steps=300,\n",
    "    death_is_not_the_end=True,\n",
    "    fixed_reset=False\n",
    ")\n",
    "cats.run()\n",
    "\n",
    "    # Visualise Training Information\n",
    "visualise_memory_mcc(\n",
    "      cats.env,\n",
    "     (cats.memory, \"teleport\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise V\n",
    "# For mountain car\n",
    "\n",
    "# states = []\n",
    "# for _ in range(10000):\n",
    "#     obs = cats.env.observation_space.sample()\n",
    "#     states.append(obs)\n",
    "# states = torch.tensor(states, device=DEVICE)\n",
    "\n",
    "X = torch.linspace(cats.env.observation_space.low[0], cats.env.observation_space.high[0], 100)\n",
    "Y = torch.linspace(cats.env.observation_space.low[1], cats.env.observation_space.high[1], 100)\n",
    "grid_X, grid_Y = torch.meshgrid((X, Y))\n",
    "\n",
    "# Time Aware\n",
    "\n",
    "states = torch.stack((grid_X.flatten(), grid_Y.flatten(), 1+torch.zeros_like(grid_X.flatten())) ).T\n",
    "#states = torch.stack((grid_X.flatten(), grid_Y.flatten())).T\n",
    "states = states.to(DEVICE)\n",
    "values = cats.V(states)\n",
    "\n",
    "states = states.cpu()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=values.min(), vmax=values.max())\n",
    "cmap = cm.viridis\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "colors = m.to_rgba(values.detach().cpu())\n",
    "\n",
    "ax.set_title(\"Value Function Visualisation\")\n",
    "ax.scatter(states[:,0], states[:, 1], c= colors)\n",
    "fig.colorbar(m, ax=ax)\n",
    "\n",
    "print(f\"Minimum Value {values.min()} | Maximum Value {values.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
