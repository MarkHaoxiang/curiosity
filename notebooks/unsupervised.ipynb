{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising MCC Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from omegaconf import DictConfig\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.autoreset import AutoResetWrapper \n",
    "\n",
    "from curiosity.experience import Transition\n",
    "from curiosity.rl.ddpg import DeepDeterministicPolicyGradient\n",
    "from curiosity.policy import ColoredNoisePolicy\n",
    "from curiosity.experience.memory import ReplayBuffer\n",
    "from curiosity.experience.util import build_replay_buffer, build_collector\n",
    "from curiosity.util.util import build_actor, build_critic, build_env, global_seed, build_intrinsic\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "COLLECTION_STEPS = 10000\n",
    "\n",
    "\n",
    "cfg = DictConfig({\n",
    "    \"seed\": 5,\n",
    "    \"env\": {\n",
    "        \"name\": \"MountainCarContinuous-v0\",\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"type\": \"experience_replay\"\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"initial_collection_size\": 128,\n",
    "        \"total_frames\": COLLECTION_STEPS,\n",
    "        \"minibatch_size\": 128   \n",
    "    },\n",
    "    \"intrinsic\": {\n",
    "        \"type\": \"disagreement\",\n",
    "        \"encoding_size\": 32,\n",
    "        \"lr\": 0.0003,\n",
    "        \"int_coef\": 1, \n",
    "        \"ext_coef\": 2,\n",
    "        \"obs_normalisation\": True,\n",
    "        \"reward_normalisation\": False,\n",
    "        \"normalised_obs_clip\": 5\n",
    "    },\n",
    "    \"noise\": {\n",
    "        \"scale\": 0.1,\n",
    "        \"beta\": 0 \n",
    "    }\n",
    "})\n",
    "\n",
    "def reset():\n",
    "    env = AutoResetWrapper(gym.make(\n",
    "        cfg.env.name,\n",
    "        render_mode=\"rgb_array\",\n",
    "        max_episode_steps=400)\n",
    "    )\n",
    "    env.reset()\n",
    "\n",
    "    rng = global_seed(cfg.seed, env)\n",
    "\n",
    "    ddpg = DeepDeterministicPolicyGradient(\n",
    "        build_actor(env),\n",
    "        build_critic(env),\n",
    "        device=DEVICE,\n",
    "        lr=0.0003\n",
    "    )\n",
    "    policy = ColoredNoisePolicy(\n",
    "        ddpg.actor,\n",
    "        env.action_space,\n",
    "        env.spec.max_episode_steps,\n",
    "        rng=rng,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    memory = build_replay_buffer(env,capacity=COLLECTION_STEPS, device=DEVICE)\n",
    "    collector = build_collector(policy, env, memory, device=DEVICE)\n",
    "    intrinsic = build_intrinsic(env, cfg.intrinsic, device=DEVICE)\n",
    "\n",
    "    return env, rng, ddpg, policy, memory, collector, intrinsic\n",
    "\n",
    "def visualise_memory_mcc(env, *memories: ReplayBuffer):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"State Space Coverage\")\n",
    "    ax.set_xlim(env.observation_space.low[0], env.observation_space.high[0])\n",
    "    ax.set_ylim(env.observation_space.low[1], env.observation_space.high[1])\n",
    "\n",
    "    for memory, name in memories:\n",
    "        batch = Transition(*memory.storage)\n",
    "        s = batch.s_0.cpu().numpy()\n",
    "        ax.scatter(s[:, 0], s[:, 1], s=2, label=name)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrinsic Only\n",
    "env, rng, ddpg, policy, memory, collector, intrinsic = reset()\n",
    "collector.early_start(cfg.train.initial_collection_size)\n",
    "batch, aux = memory.sample(cfg.train.initial_collection_size)\n",
    "intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "    collector.collect(n=1)\n",
    "    batch, aux = memory.sample(cfg.train.minibatch_size)\n",
    "    batch = Transition(*batch)\n",
    "\n",
    "    r_t, r_e, r_i = intrinsic.reward(batch)\n",
    "    intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "    batch = Transition(batch.s_0, batch.a, r_e, batch.s_1, batch.d)\n",
    "    ddpg.update(batch, aux, step=step)\n",
    "\n",
    "extrinsic_memory = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic Only\n",
    "env, rng, ddpg, policy, memory, collector, intrinsic = reset()\n",
    "collector.early_start(cfg.train.initial_collection_size)\n",
    "batch, aux = memory.sample(cfg.train.initial_collection_size)\n",
    "intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "    collector.collect(n=1)\n",
    "    batch, aux = memory.sample(cfg.train.minibatch_size)\n",
    "    batch = Transition(*batch)\n",
    "\n",
    "    r_t, r_e, r_i = intrinsic.reward(batch)\n",
    "    intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "\n",
    "    batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "    ddpg.update(batch, aux, step=step)\n",
    "\n",
    "intrinsic_memory = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Together Only\n",
    "env, rng, ddpg, policy, memory, collector, intrinsic = reset()\n",
    "collector.early_start(cfg.train.initial_collection_size)\n",
    "batch, aux = memory.sample(cfg.train.initial_collection_size)\n",
    "intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "for step in tqdm(range(1, cfg.train.total_frames+1)):\n",
    "    collector.collect(n=1)\n",
    "    batch, aux = memory.sample(cfg.train.minibatch_size)\n",
    "    batch = Transition(*batch)\n",
    "\n",
    "    r_t, r_e, r_i = intrinsic.reward(batch)\n",
    "    intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "\n",
    "    batch = Transition(batch.s_0, batch.a, r_t, batch.s_1, batch.d)\n",
    "    ddpg.update(batch, aux, step=step)\n",
    "\n",
    "sum_memory = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teleportation\n",
    "env, rng, ddpg, policy, memory, collector, intrinsic = reset()\n",
    "\n",
    "trajectory = torch.zeros((env.spec.max_episode_steps, *env.observation_space.shape), device=DEVICE)\n",
    "trajectory_index = 0\n",
    "resets = [None for _ in range(env.spec.max_episode_steps)]\n",
    "\n",
    "collector.early_start(cfg.train.initial_collection_size)\n",
    "batch, aux = memory.sample(cfg.train.initial_collection_size)\n",
    "intrinsic.initialise(Transition(*batch), aux)\n",
    "\n",
    "collector.env.reset()\n",
    "state = copy.deepcopy(collector.env)\n",
    "\n",
    "np_rng = np.random.default_rng(cfg.seed)\n",
    "\n",
    "\n",
    "for step in tqdm(range(1, cfg.train.total_frames+1)):    \n",
    "    obs, action, reward, n_obs, terminated, truncated = collector.collect(n=1)[-1]\n",
    "    # Update trajectory\n",
    "    resets[trajectory_index] = state\n",
    "    trajectory[trajectory_index] = torch.from_numpy(obs).to(DEVICE)\n",
    "    state = copy.deepcopy(collector.env)\n",
    "    trajectory_index += 1\n",
    "    # Manage Teleportation\n",
    "    if truncated or terminated:\n",
    "        # Calculate Value\n",
    "        trajectory[0] = torch.tensor(collector.env.reset()[0], device=DEVICE)\n",
    "        resets[0] = copy.deepcopy(collector.env)\n",
    "        t = trajectory[:trajectory_index]\n",
    "        target_action = ddpg.actor.target(t)\n",
    "        V = ddpg.critic.target(torch.cat((t, target_action), 1))\n",
    "        teleport_index = torch.argmax(V).item()\n",
    "        # Teleport\n",
    "        collector.env = resets[teleport_index]\n",
    "        collector.env.np_random = np.random.default_rng(np_rng.integers(65536))\n",
    "        # print(f\"===== {trajectory_index} {teleport_index} {truncated} {terminated} ====\")\n",
    "        trajectory_index = teleport_index\n",
    "        state = copy.deepcopy(collector.env)\n",
    "        \n",
    "\n",
    "    batch, aux = memory.sample(cfg.train.minibatch_size)\n",
    "    batch = Transition(*batch)\n",
    "\n",
    "    r_t, r_e, r_i = intrinsic.reward(batch)\n",
    "    intrinsic.update(batch, aux, step=step)\n",
    "\n",
    "\n",
    "    batch = Transition(batch.s_0, batch.a, r_i, batch.s_1, batch.d)\n",
    "    ddpg.update(batch, aux, step=step)\n",
    "\n",
    "teleport_memory = memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_memory_mcc(\n",
    "    env,\n",
    "    #(extrinsic_memory, \"extrinsic\"),\n",
    "     (sum_memory, \"total\"),\n",
    "    (teleport_memory, \"teleport\"),\n",
    " #   (intrinsic_memory, \"intrinsic\"),\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_memory_mcc(\n",
    "    env,\n",
    "    #(extrinsic_memory, \"extrinsic\"),\n",
    "    #(intrinsic_memory, \"intrinsic\"),\n",
    "    #(sum_memory, \"total\"),\n",
    "    (teleport_memory, \"teleport\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*intrinsic_memory.storage)\n",
    "batch.r.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*extrinsic_memory.storage)\n",
    "batch.r.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*sum_memory.storage)\n",
    "batch.r.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "obs, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    while not terminated and not truncated:\n",
    "        action = policy(obs).cpu()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(env.render())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
